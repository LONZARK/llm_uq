"""
The following code is related to task_Jul_3. 
Implementation of entropy calculation of each token in the output generated by LLM.

07-03-2024 @ Jia Li
"""

"""
The following code is related to task_Jun25_1. 
Implementation of entropy calculation of each token in the output generated by LLM.

07-01-2024 @ Jia Li
"""



from typing import List, Optional, Tuple, Dict
import fire
from llama import Dialog, Llama
import torch
from datasets import load_dataset
import os
import pickle
import time
from llama.tokenizer import Tokenizer
import numpy as np
import re

def initialize_llama3(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.6,
    top_p: float = 0.9,
    max_seq_len: int = 512,
    max_batch_size: int = 1,
    max_gen_len: Optional[int] = None,
):
    
    # Initialize the LLaMA model
    model = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )

    return model


def gen_resp_LLM(model, prompt: str, max_gen_len, top_p, temperature) -> Tuple[List[str], List[torch.Tensor]]:
    """
    This function is used to generate the response as a list of decoded tokens (not token ids).
    
    Args:
    model: the LLM model object
    prompt: a string representing the query
    
    Returns:
    Tuple[List[str], List[torch.Tensor]]: a list of decoded tokens, and a list of probability vectors representing the predictive distribution for the tokens
    """

    # Generate response
    dialog = [{"role": "user", "content": prompt}]
    response, token_logits_dict, prompt_tokens_id = model.chat_completion(
        [dialog],
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )

    decoded_tokens_id = []
    decoded_tokens = []
    prob_vectors = []
    for key, value in token_logits_dict.items():
        decoded_tokens_id.append(value['next_tokens_ids'])
        decoded_tokens.append(value['next_tokens'])
        prob_vectors.append(value['probs'])

    # tokenizer = Tokenizer(model_path=tokenizer_path)

    prompt_tokens = []
    for item in prompt_tokens_id[0]:
        prompt_tokens.append(model.tokenizer.decode([item]))

    return prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, prob_vectors



def partition_into_sentences(decoded_tokens: List[str], decoded_tokens_id: List[List[int]], prob_vectors: List[torch.Tensor]) -> List[Tuple[List[str], List[int], List[torch.Tensor]]]:
    """
    Partition the response into sentences.
    
    Args:
    decoded_tokens (List[str]): List of decoded tokens
    decoded_tokens_id (List[List[int]]): List of token IDs corresponding to decoded_tokens
    prob_vectors (List[torch.Tensor]): List of probability vectors for each token

    Returns:
    List[Tuple[List[str], List[int], List[torch.Tensor]]]: List of sentences, where each sentence is a tuple containing
        (list of tokens, list of token IDs, list of probability vectors)
    """
    sentences = []
    current_sentence_tokens = []
    current_sentence_ids = []
    current_sentence_probs = []

    # Iterate over the tokens and their corresponding prob_vector
    for token, token_id, prob_vector in zip(decoded_tokens, decoded_tokens_id, prob_vectors):
        # Check if the current token contains a newline character
        if '\n' in token:
            # Split the token at each newline character
            sub_tokens = token.split('\n')
            
            for i, sub_token in enumerate(sub_tokens):
                if sub_token:  # If the sub_token is not empty
                    current_sentence_tokens.append(sub_token)
                    current_sentence_ids.extend(token_id)
                    current_sentence_probs.append(prob_vector)

                # If it's not the last sub_token, end the current sentence
                if i < len(sub_tokens) - 1:
                    sentences.append((current_sentence_tokens, current_sentence_ids, current_sentence_probs))
                    current_sentence_tokens = []
                    current_sentence_ids = []
                    current_sentence_probs = []
        else:
            # If no newline, just add the token to the current sentence
            current_sentence_tokens.append(token)
            current_sentence_ids.extend(token_id)
            current_sentence_probs.append(prob_vector)

    # Add any remaining tokens as the last sentence
    if current_sentence_tokens:
        sentences.append((current_sentence_tokens, current_sentence_ids, current_sentence_probs))

    current_sentence_tokens = []
    current_sentence_ids = []
    current_sentence_probs = []
    for item in sentences:
        current_sentence_tokens.append(sentences[0])
        current_sentence_ids.append(sentences[1])
        current_sentence_probs.append(sentences[2])


    return sentences

def get_samples_MC(model, prompt: List[str], prompt_id, primary_response, primary_response_id, sentence: List[str], n_samples: int) -> List[Dict]:
    """
    Get Monte Carlo samples and their probabilities for a given sentence.
    Perform Monte-Carlo sampling for a sentence in the primary response of the model given a prompt.

    Args:
    model: LLM model object
    prompt (List[str]): The input prompt, a list of tokens
    primary_response (List[str]): The primary response (response when temperature=0), a list of tokens
    sentence (List[str]): A sentence in the primary response, a list of tokens
    n_samples (int): Number of Monte Carlo samples to generate

    Returns:
    List[Dict]: A list of dictionaries, each containing a sample sentence and its sequence probability
    """
    # Find the index of the sentence in the primary response
    sentence_start_index = primary_response.index(sentence[0][0])
    
    # Construct the context by combining the prompt and the text up to the current sentence
    c = [item[0] for item in primary_response_id[:sentence_start_index]]
    context = [prompt_id[0]+c]
    print('context', context)
    
    samples = []
    
    for _ in range(n_samples):
        # Generate a new sample using the model with non-zero temperature
       
        generation_tokens, generation_logprobs, token_logits_dict = model.generate(
            prompt_tokens=context,
            max_gen_len=512,
            temperature= 0.99,
            # logprobs=logprobs,
        ) 

        decoded_tokens_id = []
        decoded_tokens = []
        prob_vectors = []
        for key, value in token_logits_dict.items():
            decoded_tokens_id.append(value['next_tokens_ids'])
            decoded_tokens.append(value['next_tokens'])
            prob_vectors.append(value['probs'])

        sentences = partition_into_sentences(decoded_tokens, decoded_tokens_id, prob_vectors)
        samples.append(sentences[0])

    return samples


def calculate_mcse(samples) -> float:
    """
    Calculates the Monte Carlo Sequence Entropy (MCSE) for a given set of samples and their probabilities.

    Args:
    samples (List[List[str]]): A list of token lists, each representing a sample
    sample_probs (List[float]): A list of probabilities corresponding to each sample

    Returns:
    float: The calculated Monte Carlo Sequence Entropy
    """
    # Convert probabilities to log probabilities
    sample_probs = [item[2] for item in samples]

    sample_probs_numpy = []

    for item in sample_probs[0]:
        if item.is_cuda:
            item = item.cpu()
        item = item.numpy()
        sample_probs_numpy.append(item)

    log_probs = np.log(sample_probs_numpy)
    log_probs = log_probs.squeeze()
    
    # Calculate the negative mean of log probabilities
    mcse = np.array([-np.mean(lp) for lp in log_probs])

    return mcse


def cal_MCSE(model, prompt: List[str], n_samples: int) -> List[Dict]:
    """
    Calculate MCSE (Monte Carlo Sequence Entropy) uncertainty score given the model and prompt.
    
    Args:
    model: LLM model object
    prompt (List[str]): The input prompt, a list of tokens
    n_samples (int): The setting of sample size in Monte-Carlo sampling
    
    Returns:
    List[Dict]: A list of dictionaries containing MCSE scores for each sentence
    """
    # Step 1: Generate the primary response
    prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, prob_vectors = gen_resp_LLM(model, prompt, max_gen_len = 256, top_p = 0, temperature = 0) 
    
    # Step 2: Partition the primary response into sentences
    sentences = partition_into_sentences(decoded_tokens, decoded_tokens_id, prob_vectors)

    results = []

    # Step 3: Calculate MCSE for each sentence
    for sentence in sentences:
        # Get Monte Carlo samples and their probabilities
        if len(sentence[0]) == 0:
            pass
        else:
            mc_samples = get_samples_MC(model, prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, sentence,  n_samples =5)

            # Calculate MCSE
            mcse = calculate_mcse(mc_samples)
            
            # Store results
            results.append({
                "sentence": sentence,
                "MC_samples": mc_samples,
                "MCSE": mcse
            })

    return results


def calculate_mcnse(samples) -> float:
    """
    Calculates the Monte Carlo Sequence Entropy (MCSE) for a given set of samples and their probabilities.

    Args:
    samples (List[List[str]]): A list of token lists, each representing a sample
    sample_probs (List[float]): A list of probabilities corresponding to each sample

    Returns:
    float: The calculated Monte Carlo Sequence Entropy
    """
    # Convert probabilities to log probabilities
    sample_probs = [item[2] for item in samples]
    tokens = [item[1] for item in samples]

    sample_probs_numpy = []
    for item in sample_probs[0]:
        if item.is_cuda:
            item = item.cpu()
        item = item.numpy()
        sample_probs_numpy.append(item)

    log_probs = np.log(sample_probs_numpy)
    log_probs = log_probs.squeeze()

    # Calculate the negative mean of log probabilities
    # Calculate the normalized entropy for each pair of token log probabilities and token list
    mcnse = np.array([
        -np.mean([lp_i / len(t) for lp_i in lp])  # Normalize by the length of tokens list for each sample
        for lp, t in zip(log_probs, tokens)
    ])
    return mcnse

def cal_MCNSE(model, prompt: List[str], n_samples: int) -> List[Dict]:
    """
    Calculate MCNSE (Monte Carlo Normalized Sequence Entropy) uncertainty score given the model and prompt.
    
    Args:
    model: LLM model object
    prompt (List[str]): The input prompt, a list of tokens
    n_samples (int): The setting of sample size in Monte-Carlo sampling
    
    Returns:
    List[Dict]: A list of dictionaries containing MCSE scores for each sentence
    """
    # Step 1: Generate the primary response
    prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, prob_vectors = gen_resp_LLM(model, prompt, max_gen_len = 256, top_p = 0, temperature = 0) 
    
    # Step 2: Partition the primary response into sentences
    sentences = partition_into_sentences(decoded_tokens, decoded_tokens_id, prob_vectors)

    results = []

    # Step 3: Calculate MCSE for each sentence
    for sentence in sentences:
        # Get Monte Carlo samples and their probabilities
        if len(sentence[0]) == 0:
            pass
        else:
            mc_samples = get_samples_MC(model, prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, sentence,  n_samples =5)

            # Calculate MCSE
            mcse = calculate_mcnse(mc_samples)
            
            # Store results
            results.append({
                "sentence": sentence,
                "MC_samples": mc_samples,
                "MCSE": mcse
            })

    return results




def main(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.6,
    top_p: float = 0,
    max_seq_len: int = 512,
    max_batch_size: int = 22,
    max_gen_len: Optional[int] = 256,
):
    
    model = initialize_llama3(ckpt_dir, tokenizer_path, temperature, top_p, max_seq_len, max_batch_size, max_gen_len)
    prompt = """ Who is Donald Trump? Answer with each sentence in a new line. """

    prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, prob_vectors = gen_resp_LLM(model, prompt, max_gen_len, top_p, temperature) 

    # # print('============ prompt_tokens =============')    
    # # print('prompt_tokens', prompt_tokens)    
    # # print('============ prompt_tokens_id =============')    
    # # print('prompt_tokens_id', prompt_tokens_id)    
    # # print('============ decoded_tokens ============')    
    # # print('decoded_tokens', decoded_tokens)   
    # # print('============ decoded_tokens_id =============')    
    # # print('decoded_tokens_id', decoded_tokens_id)    
    # # print('============ prob_vectors =============')    
    # # print('prob_vectors', prob_vectors)    

    sentences = partition_into_sentences(decoded_tokens, decoded_tokens_id, prob_vectors)
    # print('============ sentences =============')    
    # print('sentences', sentences)    

    for sentence in sentences:
        # Get Monte Carlo samples and their probabilities
        if len(sentence[0]) == 0:
            pass
        else:
            samples = get_samples_MC(model, prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, sentence,  n_samples =5)
            print('============ samples =============')    
            print('samples', samples)
            
            mcse = calculate_mcse(samples)
            print('mcse', mcse)

            mcnse = calculate_mcnse(samples)
            print('mcnse', mcnse)


    # cal_MCNSE_results = cal_MCNSE(model, prompt, 3)
    # print('cal_MCNSE results', cal_MCNSE_results)

    # cal_MCSE_results = cal_MCSE(model, prompt, 3)
    # print('cal_MCSE results', cal_MCSE_results)


if __name__ == "__main__":
    fire.Fire(main)

# conda activate llmuq; cd /people/cs/j/jxl220096/llmuq/llama3/; torchrun --nproc_per_node 1 task_Jul_3.py --ckpt_dir Meta-Llama-3-8B-Instruct/ --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 6

