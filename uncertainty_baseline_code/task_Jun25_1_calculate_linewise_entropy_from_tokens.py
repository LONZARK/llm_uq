"""
The following code is related to task_Jun25_1. 
Implementation of entropy calculation of each token in the output generated by LLM.

07-01-2024 @ Jia Li
"""



from typing import List, Optional, Tuple
import fire
from llama import Dialog, Llama
import torch
from datasets import load_dataset
import os
import pickle
import time
from llama.tokenizer import Tokenizer


def initialize_llama3(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.6,
    top_p: float = 0.9,
    max_seq_len: int = 512,
    max_batch_size: int = 1,
    max_gen_len: Optional[int] = None,
):
    
    # Initialize the LLaMA model
    model = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )

    return model


def gen_resp_LLM(model, prompt: str, max_gen_len, top_p, temperature) -> Tuple[List[str], List[torch.Tensor]]:
    """
    This function is used to generate the response as a list of decoded tokens (not token ids).
    
    Args:
    model: the LLM model object
    prompt: a string representing the query
    
    Returns:
    Tuple[List[str], List[torch.Tensor]]: a list of decoded tokens, and a list of probability vectors representing the predictive distribution for the tokens
    """

    # Generate response
    dialog = [{"role": "user", "content": prompt}]
    response, token_logits_dict, prompt_tokens_id = model.chat_completion(
        [dialog],
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )

    decoded_tokens_id = []
    decoded_tokens = []
    prob_vectors = []
    for key, value in token_logits_dict.items():
        decoded_tokens_id.append(value['next_tokens_ids'])
        decoded_tokens.append(value['next_tokens'])
        prob_vectors.append(value['probs'])

    # tokenizer = Tokenizer(model_path=tokenizer_path)

    prompt_tokens = []
    for item in prompt_tokens_id[0]:
        prompt_tokens.append(model.tokenizer.decode([item]))

    return decoded_tokens, prob_vectors, prompt_tokens, prompt_tokens_id, decoded_tokens_id




def calculate_entropy(prob_vec: torch.Tensor) -> float:
    """
    This function calculates the entropy from a probability vector that represents a categorical distribution.
    
    The input probability vector should be normalized to be valid. For example, if the input probability vector 
    representing predicted probability of Top-3 choices is [0.5, 0.1, 0.2], it should be normalized to [0.625, 0.125, 0.25].
    
    Args:
    prob_vec: a probability vector or dictionary that stores the predicted probabilities of some next token
    
    Returns:
    List[float]: a list of scalar values representing the entropy for each probability vector.
    """
    entropy_list = []

    for prob in prob_vec:
        # Ensure the probabilities are normalized
        prob = prob / prob.sum(dim=-1, keepdim=True)
        # Calculate entropy
        entropy = -torch.sum(prob * torch.log(prob + 1e-9), dim=-1)  # Adding a small value to avoid log(0)
        entropy_list.append(entropy.item())

    return entropy_list


def calculate_tokenwise_entropy(model, prompt: str, max_gen_len, top_p, temperature):
    """
    This function generates the primary response, as well as the token-wise entropy for each token in the response.
    
    Args:
    model: the LLM model object
    prompt: a string representing the querys
    temperature: the parameter that controls the randomness in the response
    top_p: the parameter for nucleus sampling
    max_gen_len: maximum sequence length for the output
    
    Returns:
    Tuple[List[str], List[float]]: a list of tokens representing the primary response, and a list of token-wise entropies
    """

    decoded_tokens, prob_vectors = gen_resp_LLM(model, prompt, max_gen_len, top_p, temperature)
    entropy_list = calculate_entropy(prob_vectors)
    assert len(decoded_tokens) == len(entropy_list), "The lengths of decoded_tokens and entropy_list do not match"

    return zip(decoded_tokens, entropy_list)

def calculate_linewise_entropy_from_tokens(model, prompt: str, max_gen_len, top_p, temperature):
    """
    This function generates the primary response, as well as the line-wise entropy for each token in the response.
    The line-wise entropy for a line is calculated as the average token-wise entropy for all the tokens in it after
    all tokens and entropies have been collected for that line.

    Args:
    model: the LLM model object
    prompt: a string representing the querys
    temperature: the parameter that controls the randomness in the response
    top_p: the parameter for nucleus sampling
    max_gen_len: maximum sequence length for the output
    
    Returns:
        List[Tuple[str, float]]: A list of tuples, each containing a line of text as a string and the average entropy 
        of that line as a float.
    """
    prompt_tokens, prompt_tokens_id, decoded_tokens, decoded_tokens_id, prob_vectors = gen_resp_LLM(model, prompt, max_gen_len = 256, top_p = 0, temperature = 0)
    token_entropy_list = calculate_entropy(prob_vectors)
    assert len(decoded_tokens) == len(token_entropy_list), "The lengths of decoded_tokens and entropy_list do not match"

    # Initialize empty lists to store lines and their corresponding lists of entropies
    lines = []
    entropies_per_line = []
    # Initialize temporary lists to accumulate tokens and entropies for the current line
    current_line = []
    current_entropies = []

    # Iterate over the tokens and their corresponding entropies
    for token, entropy in zip(decoded_tokens, token_entropy_list):
        # Check if the current token contains a newline character
        if '\n' in token:
            # Split the token at each newline character
            parts = token.split('\n')

            # Iterate over each part of the token split by newline
            for i, part in enumerate(parts):
                if i < len(parts) - 1:
                    # Add to the current line
                    current_line.append(part)
                    current_entropies.append(entropy)
                    # Combine and store the current line and its list of entropies, then reset for a new line
                    lines.append(''.join(current_line))
                    entropies_per_line.append(current_entropies)
                    current_line = []
                    current_entropies = []
                else:
                    # Continue adding to the current line
                    current_line.append(part)
                    current_entropies.append(entropy)
        else:
            # If no newline in token, add token and entropy to the current line
            current_line.append(token)
            current_entropies.append(entropy)

    # After the loop, if there's any remaining data in the current line, add it to the final lists
    if current_line:
        lines.append(''.join(current_line))
        entropies_per_line.append(current_entropies)

    line_entropies = [sum(ents) / len(ents) if ents else 0 for ents in entropies_per_line]
    result = list(zip(lines, line_entropies))
    return result



def main(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.6,
    top_p: float = 0,
    max_seq_len: int = 512,
    max_batch_size: int = 22,
    max_gen_len: Optional[int] = 256,
):
    
    model = initialize_llama3(ckpt_dir, tokenizer_path, temperature, top_p, max_seq_len, max_batch_size, max_gen_len)
    prompt = """Hello, can you introduce Trump?"""

    decoded_tokens, prob_vectors, prompt_tokens, prompt_tokens_id, decoded_tokens_id = gen_resp_LLM(model, prompt, max_gen_len, top_p, temperature) 

    # print('============ prob_vectors =============')    
    # print('prob_vectors', prob_vectors)    
    print('============ prompt_tokens =============')    
    print('prompt_tokens', prompt_tokens)    
    print('============ decoded_tokens ============')    
    print('decoded_tokens', decoded_tokens)   
    print('============ decoded_tokens_id =============')    
    print('decoded_tokens_id', decoded_tokens_id)    


    c = [item[0] for item in decoded_tokens_id[:5]]
    print('c', c)
    print('prompt_tokens_id[0]', prompt_tokens_id[0])
    next_prompt_tokens = [prompt_tokens_id[0]+c]

    generation_tokens, generation_logprobs, token_logits_dict = model.generate(
        prompt_tokens=next_prompt_tokens,
        max_gen_len=max_gen_len,
        temperature= 0.6,
        top_p=top_p,
        # logprobs=logprobs,
    ) 

    decoded_tokens_id = []
    decoded_tokens = []
    prob_vectors = []
    for key, value in token_logits_dict.items():
        decoded_tokens_id.append(value['next_tokens_ids'])
        decoded_tokens.append(value['next_tokens'])
        prob_vectors.append(value['probs'])

    # tokenizer = Tokenizer(model_path=tokenizer_path)  
    print('============ decoded_tokens ============')    
    print('decoded_tokens', decoded_tokens)   
    print('============ decoded_tokens_id =============')    
    print('decoded_tokens_id', decoded_tokens_id)    



    # token_entropy_list  = calculate_entropy(prob_vectors)

    # # print('============ token_entropy_list =============')    
    # # print('token_entropy_list ', token_entropy_list )    
    # tokenwise_entropy = calculate_tokenwise_entropy(model, prompt, max_gen_len, top_p, temperature)
    # line_entropies = calculate_linewise_entropy_from_tokens(model, prompt, max_gen_len, top_p, temperature)

    # print('============ tokenwise_entropy =============')    
    # for i in tokenwise_entropy:
    #     print(i)
    # print('============ line_entropies =============')    
    # for i in line_entropies:
    #     print(i)


if __name__ == "__main__":
    fire.Fire(main)

# conda activate llmuq; cd /people/cs/j/jxl220096/llmuq/llama3/; torchrun --nproc_per_node 1 task_Jun25_1.py --ckpt_dir Meta-Llama-3-8B-Instruct/ --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4
