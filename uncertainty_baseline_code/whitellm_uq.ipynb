{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxl220096/anaconda3/envs/llmuq/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from utils.generation import is_chat\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "def is_chat(obj):\n",
    "    \"\"\"\n",
    "    Determines if the given object is a properly formatted chat.\n",
    "\n",
    "    Args:\n",
    "    messages (list): A list of dictionaries representing chat messages.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the object is a chat, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if messages is a list\n",
    "    if not isinstance(obj, list):\n",
    "        return False\n",
    "    \n",
    "    # Check if each item in the list is a dictionary with 'role' and 'content' keys\n",
    "    for message in obj:\n",
    "        if not isinstance(message, dict):\n",
    "            return False\n",
    "        if 'role' not in message or 'content' not in message:\n",
    "            return False\n",
    "        if not isinstance(message['role'], str) or not isinstance(message['content'], str):\n",
    "            return False\n",
    "        if message['role'] not in ['system', 'user', 'assistant']:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def set_device(device='cuda:0'):\n",
    "    # Check if the specified device is a GPU and parse its index\n",
    "    if device.startswith('cuda'):\n",
    "        # Try to parse out the GPU index after 'cuda:'\n",
    "        gpu_index = device.split(':')[-1]\n",
    "        try:\n",
    "            gpu_index = int(gpu_index)\n",
    "            if gpu_index >= torch.cuda.device_count() or not torch.cuda.is_available():\n",
    "                raise ValueError(\"GPU index out of range or CUDA is not available.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Specified device '{device}' is not available. Error: {e}\")\n",
    "            device = 'cpu'\n",
    "    elif device not in ['cpu']:\n",
    "        print(f\"Invalid device '{device}' specified. Falling back to 'cpu'.\")\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "def print_gpu_memory_usage():\n",
    "    mem_allocated = torch.cuda.memory_allocated()/1024**3\n",
    "    mem_reserved = torch.cuda.memory_reserved()/1024**3\n",
    "    print(\"Allocated memory:\", mem_allocated, \"GB\")\n",
    "    print(\"Cached memory:\", mem_reserved, \"GB\")\n",
    "    # return mem_allocated, mem_reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteBoxLLM:\n",
    "    VALID_MODEL_TYPES = [\"AutoModelForCausalLM\"]\n",
    "    VALID_UQ_METHODS = [\"MSP\", \"MTE\", \"PP\", \"P(True)\", \"MCSE\", \"MCNSE\", \"PMI\", \"CPMI\"]\n",
    "\n",
    "    def __init__(self, model_type, use_multi_gpu=False):\n",
    "        if model_type not in self.VALID_MODEL_TYPES:\n",
    "            raise ValueError(f\"Invalid model type. Choose from {self.VALID_MODEL_TYPES}\")\n",
    "        self.use_multi_gpu = use_multi_gpu\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.pipeline=None\n",
    "        self.tokenizer = None\n",
    "        self.device = None\n",
    "        self.dtype = torch.bfloat16\n",
    "        \n",
    "    def load_pretrained(self, model_path, device='cpu', dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        Load a pretrained model and tokenizer.\n",
    "\n",
    "        Args:\n",
    "        model_path (str): Path to the pretrained model.\n",
    "        device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "        dtype (torch.dtype): Data type to use for the model.\n",
    "        \"\"\"\n",
    "        print(f\"loading pretrained model from: {model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.dtype = dtype\n",
    "        self.device = set_device(device=device)\n",
    "        \n",
    "        if self.use_multi_gpu:\n",
    "            ### use multiple GPUs with transformers.pipeline\n",
    "            start = time.time()\n",
    "            self.pipeline = transformers.pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model_path,\n",
    "                model_kwargs={\"torch_dtype\": self.dtype},\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            self.tokenizer = self.pipeline.tokenizer\n",
    "            self.model = self.pipeline.model\n",
    "            end = time.time()\n",
    "            print(f\"Pipeline constructed in {str(end-start)} seconds. Model is loaded from path:\\n {model_path}.\")\n",
    "        else:\n",
    "            ### load model on device\n",
    "            start = time.time()\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device, dtype=self.dtype)    \n",
    "            print(f\"loading {type(self.model)}\")\n",
    "            end = time.time()\n",
    "            print(f\"Model loaded from path: {model_path}.\")\n",
    "            print(f\"Time: {str(end-start)} seconds\")  \n",
    "        \n",
    "    def tokenize_input(self, prompt, mode=\"text\", add_special_tokens=True, tokenize=True):\n",
    "        \"\"\"\n",
    "        Tokenize the input prompt.\n",
    "\n",
    "        Args:\n",
    "        prompt (str or list): The input prompt.\n",
    "        mode (str): Mode of tokenization, either 'text' or 'chat'.\n",
    "        add_special_tokens (bool): Whether to add special tokens.\n",
    "        tokenize (bool): If True, returns tokenized input; if False, returns token values.\n",
    "\n",
    "        Returns:\n",
    "        dict or list: Tokenized input if `tokenize` is True, otherwise list of token values.\n",
    "        \"\"\"\n",
    "        if mode not in [\"text\", \"chat\"]:\n",
    "            raise ValueError(\"Mode must be either 'text' or 'chat'\")\n",
    "\n",
    "        if mode == \"text\":\n",
    "            if not isinstance(prompt, str):\n",
    "                raise ValueError(\"Prompt must be a string for text mode.\")\n",
    "            \n",
    "        elif mode == \"chat\":\n",
    "            ### generation prompt will be added\n",
    "            if not isinstance(prompt, list) or not all(isinstance(msg, dict) and 'role' in msg and 'content' in msg for msg in prompt):\n",
    "                raise ValueError(\"Prompt must be a list of dictionaries with 'role' and 'content' keys for chat mode.\")\n",
    "            prompt = self.tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            return_tensors='pt' if tokenize else None\n",
    "        )\n",
    "\n",
    "        if tokenize:\n",
    "            return encoding  # Returns tokenized input\n",
    "        else:\n",
    "            return self.tokenizer.convert_ids_to_tokens(encoding.input_ids[0])  # Returns list of token values\n",
    "\n",
    "    def generate(self, input, max_new_tokens=512, temperature=1.0, top_p=0.9, top_k=50, do_sample=False, return_type='raw'):\n",
    "        \"\"\"\n",
    "        Generate text using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            input (dict, list, or str): Tokenized input containing input_ids and attention_mask(optional), or a chat object, or plain text.\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_p (float): Top-p sampling.\n",
    "            top_k (int): Top-k sampling.\n",
    "            do_sample (bool): Whether to use sampling; use greedy decoding otherwise.\n",
    "            return_in_dict (bool): If True, return detailed information in a dictionary.\n",
    "            \n",
    "        Returns:\n",
    "            str or dict: raw output of model.generate() or a dictionary with detailed results if return_in_dict is True.\n",
    "        \"\"\"\n",
    "        VALID_RETURN_TYPES = ['generation_text', 'generation_dict', 'pred_logprobs', 'raw']\n",
    "        \n",
    "        if return_type not in VALID_RETURN_TYPES:\n",
    "            print(f\"Invalid return type '{return_type}' specified. Valid return types: {str(VALID_RETURN_TYPES)}. Falling back to 'generation_text'.\")\n",
    "            return_type = 'generation_text'\n",
    "\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        terminators = self.tokenizer.eos_token_id\n",
    "        \n",
    "        \n",
    "        # Handling different input types\n",
    "        if isinstance(input, dict):  # Input is a dict, as the direct input of model.generate()\n",
    "            if 'input_ids' not in input:\n",
    "                raise ValueError(\"The input dictionary must contain 'input_ids'.\")\n",
    "            tokenized_input = input\n",
    "        elif isinstance(input, list):  # Assuming input is a chat object\n",
    "            if not is_chat(input):\n",
    "                raise ValueError(\"The list input must be a valid chat format.\")\n",
    "            tokenized_input = self.tokenize_input(input, mode='chat', add_special_tokens=True, tokenize=True)\n",
    "        elif isinstance(input, str):  # Input is a plain text string\n",
    "            tokenized_input = self.tokenize_input(input, mode='text', add_special_tokens=True, tokenize=True)\n",
    "        else:\n",
    "            raise ValueError(\"Input format not supported. It must be either a dict, list (chat), or string (text).\")\n",
    "\n",
    "        # set attention masks\n",
    "        if 'attention_mask' not in input:\n",
    "            tokenized_input['attention_mask'] = tokenized_input['input_ids'].ne(self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        input_ids = tokenized_input['input_ids'].to(self.model.device)\n",
    "        attention_mask = tokenized_input['attention_mask'].to(self.model.device)\n",
    "\n",
    "        # Model inference\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        if return_type=='raw':\n",
    "            return output\n",
    "        \n",
    "        if return_type=='pred_logprobs':\n",
    "            ### return the predicted log-probs of new tokens generated in a tuple\n",
    "            ### apply exponential to get the predicted probability vector for each newly generated token\n",
    "            ### further apply max on predicted probability vector to get the predicted probability of greedily selected token\n",
    "            pred_logprobs = tuple(torch.nn.functional.log_softmax(score, dim=1) for score in output.scores)\n",
    "            return pred_logprobs\n",
    "        \n",
    "        \n",
    "        input_len = input_ids.size(1)\n",
    "        \n",
    "        output_ids = output.sequences[0]  \n",
    "        new_ids = output_ids[input_len:]\n",
    "        \n",
    "        new_text = self.tokenizer.decode(new_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        \n",
    "        if return_type=='generation_text':\n",
    "            ### return the generated text\n",
    "            return new_text\n",
    "        \n",
    "        \n",
    "        input_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        output_text = self.tokenizer.decode(output_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        output_tokens = self.tokenizer.convert_ids_to_tokens(output_ids)\n",
    "        new_tokens = self.tokenizer.convert_ids_to_tokens(new_ids)\n",
    "           \n",
    "        if return_type=='generation_dict':\n",
    "            ### return the generation dict with rich information\n",
    "            return {\n",
    "                \"paras\": {\n",
    "                    \"encoding\": {\"add_special_tokens\": True},\n",
    "                    \"generation\": {\n",
    "                        \"max_new_tokens\": max_new_tokens,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"top_p\": top_p,\n",
    "                        \"top_k\": top_k,\n",
    "                        \"do_sample\": do_sample\n",
    "                    },\n",
    "                    \"decoding\": {\"skip_special_tokens\": False, \"clean_up_tokenization_spaces\": False}\n",
    "                },\n",
    "                \"input_ids\": input_ids[0].tolist(),\n",
    "                \"new_ids\": new_ids.tolist(),\n",
    "                \"output_ids\": output_ids.tolist(),\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"new_tokens\": new_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"input_text\": input_text,\n",
    "                \"new_text\": new_text,\n",
    "                \"output_text\": output_text,\n",
    "                \"tokenized_input\": {\n",
    "                    \"input_ids\": input_ids[0].tolist(),\n",
    "                    \"attention_mask\": attention_mask[0].tolist()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def batch_generate():\n",
    "        pass\n",
    "\n",
    "    def generate_uncertainty_scores(self, generation_dict, pred_logprobs, uq_methods=None):\n",
    "        \"\"\"\n",
    "        Generate uncertainty scores with the specified uncertainty quantification methods.\n",
    "\n",
    "        Args:\n",
    "        generation_dict (dict): Dictionary containing generation information.\n",
    "        uq_methods (list): List of strings containing the names for the methods.\n",
    "\n",
    "        Returns:\n",
    "        dict: Uncertainty dictionary with generated uncertainty scores.\n",
    "        \"\"\"\n",
    "        if uq_methods is None:\n",
    "            uq_methods = self.VALID_UQ_METHODS\n",
    "\n",
    "        # Check for invalid methods and remove them from uq_methods\n",
    "        invalid_methods = [method for method in uq_methods if method not in self.VALID_UQ_METHODS]\n",
    "\n",
    "        if invalid_methods:\n",
    "            print(f\"Invalid UQ methods: {invalid_methods}. They will be ignored.\")\n",
    "            uq_methods = [method for method in uq_methods if method in self.VALID_UQ_METHODS]\n",
    "        print(f\"Valid UQ methods to execute: {uq_methods}\")\n",
    "\n",
    "        # Create a copy of the generation_dict to uncertainty_dict\n",
    "        uncertainty_dict = generation_dict.copy()\n",
    "\n",
    "        # Iterate over uq_methods and call the corresponding method\n",
    "        for method in uq_methods:\n",
    "            # Call the method (dummy implementation)\n",
    "            getattr(self, method.lower().replace('(', '').replace(')', ''))(uncertainty_dict, pred_logprobs)\n",
    "\n",
    "        return uncertainty_dict\n",
    "\n",
    "\n",
    "    def split_lines(self, tokens):\n",
    "        token_split = []\n",
    "        cur_split = []\n",
    "\n",
    "        for token in tokens:\n",
    "            cur_split.append(token)\n",
    "            if '\\n' in token:\n",
    "                token_split.append(cur_split)\n",
    "                cur_split = []\n",
    "\n",
    "        # Append any remaining tokens that did not end with a newline\n",
    "        if cur_split:\n",
    "            token_split.append(cur_split)\n",
    "\n",
    "        # print(\"Total tokens after split:\", sum(len(split) for split in token_split))\n",
    "\n",
    "        # Create list of lines by joining tokens and removing newlines\n",
    "        line_split = [''.join(split).replace('\\n', '') for split in token_split]\n",
    "\n",
    "        return line_split, token_split\n",
    "    \n",
    "    def msp(self, uncertainty_dict, pred_logprobs):\n",
    "        new_ids = uncertainty_dict['new_ids']\n",
    "        \n",
    "        # Concatenate all probability tensors into a single tensor for batch processing\n",
    "        logits = torch.cat([log_prob.squeeze(0) for log_prob in pred_logprobs], dim=0)\n",
    "        # Convert log probabilities to probabilities\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Calculate the maximum probability per token\n",
    "        max_probs = probabilities.max(dim=1).values\n",
    "        \n",
    "        # Global MSP score\n",
    "        global_prob_product = torch.prod(max_probs)\n",
    "        global_msp = 1 - global_prob_product.item()\n",
    "        \n",
    "        if 'global_unc_score' not in uncertainty_dict:\n",
    "            uncertainty_dict['global_unc_score'] = {}\n",
    "        uncertainty_dict['global_unc_score'].update({'MSP': global_msp})\n",
    "\n",
    "        # Linewise MSP scores\n",
    "        line_split, token_split = self.split_lines(uncertainty_dict['new_text'])\n",
    "\n",
    "        if 'line_unc_scores' not in uncertainty_dict:\n",
    "            uncertainty_dict['line_unc_scores'] = []\n",
    "\n",
    "        current_token_index = 0\n",
    "        for i, line_tokens in enumerate(token_split):\n",
    "            line_length = len(line_tokens)\n",
    "            line_text = line_split[i]\n",
    "\n",
    "            line_token_ids = new_ids[current_token_index:current_token_index + line_length]\n",
    "            line_probs = max_probs[current_token_index:current_token_index + line_length]\n",
    "            line_prob_product = torch.prod(line_probs)\n",
    "            line_msp = 1 - line_prob_product.item()\n",
    "\n",
    "            new_scores = {\"MSP\": line_msp}\n",
    "            existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "\n",
    "            if existing_line:\n",
    "                existing_line[2].update(new_scores)\n",
    "            else:\n",
    "                uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "            current_token_index += line_length\n",
    "\n",
    "        return uncertainty_dict\n",
    "\n",
    "\n",
    "    def mte(self, uncertainty_dict, pred_logprobs):\n",
    "        new_ids = uncertainty_dict['new_ids']\n",
    "        \n",
    "        # Convert list of log probabilities tensors to a single tensor\n",
    "        logits = torch.cat([log_prob.squeeze(0) for log_prob in pred_logprobs], dim=0)\n",
    "        \n",
    "        # Convert log probabilities to probabilities\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Calculate entropy for each token\n",
    "        entropies = -(probabilities * torch.log2(probabilities)).sum(dim=-1)\n",
    "        global_mte = entropies.mean().item()\n",
    "\n",
    "        if 'global_unc_score' not in uncertainty_dict:\n",
    "            uncertainty_dict['global_unc_score'] = {}\n",
    "        uncertainty_dict['global_unc_score'].update({'MTE': global_mte})\n",
    "\n",
    "        # Linewise MTE\n",
    "        line_split, token_split = self.split_lines(uncertainty_dict['new_text'])\n",
    "        \n",
    "        if 'line_unc_scores' not in uncertainty_dict:\n",
    "            uncertainty_dict['line_unc_scores'] = []\n",
    "\n",
    "        current_token_index = 0\n",
    "        for i, line_tokens in enumerate(token_split):\n",
    "            line_length = len(line_tokens)\n",
    "            line_text = line_split[i]\n",
    "\n",
    "            line_token_ids = new_ids[current_token_index:current_token_index + line_length]\n",
    "            line_probs = probabilities[current_token_index:current_token_index + line_length]\n",
    "            token_entropies = -(line_probs * torch.log2(line_probs)).sum(dim=-1)\n",
    "\n",
    "            line_mean_entropy = token_entropies.mean().item()\n",
    "            new_scores = {\"MTE\": line_mean_entropy}\n",
    "            existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "\n",
    "            if existing_line:\n",
    "                existing_line[2].update(new_scores)\n",
    "            else:\n",
    "                uncertainty_dict['line_unc_scores'].append((line_text, line_token_ids, new_scores))\n",
    "            current_token_index += line_length\n",
    "\n",
    "        return uncertainty_dict\n",
    "\n",
    "    def pp(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def ptrue(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def mcse(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def mcnse(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def pmi(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def cpmi(self, uncertainty_dict, pred_logprobs):\n",
    "        pass\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False):\n",
    "        \"\"\"\n",
    "        Decode the token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "        token_ids (list or torch.Tensor): Token IDs to decode.\n",
    "        skip_special_tokens (bool): Whether to skip special tokens.\n",
    "        clean_up_tokenization_spaces (bool): Whether to clean up tokenization spaces.\n",
    "\n",
    "        Returns:\n",
    "        str: Decoded string.\n",
    "        \"\"\"\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.tolist()\n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens, clean_up_tokenization_spaces=clean_up_tokenization_spaces)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model from: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxl220096/anaconda3/envs/llmuq/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jxl220096/anaconda3/envs/llmuq/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Model loaded from path: TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "Time: 2.0247466564178467 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxl220096/anaconda3/envs/llmuq/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"evalplus/humanevalplus\")\n",
    "user_message = dataset['test'][0]['prompt']\n",
    "task_id = dataset['test'][0]['task_id']\n",
    "\n",
    "system_message = '''You are a Python code generator. Generate a complete and functioning Python function based on the provided code snippet.\n",
    "Ensure the function includes the original instructions in the comments, in-line comments for each line of code, and import statements for any required dependencies.\n",
    "Do not include main function. Enclose your code inside a ```python``` block.'''\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "llm = WhiteBoxLLM('AutoModelForCausalLM')\n",
    "llm.load_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "tokenized_input = llm.tokenize_input(chat, mode='chat', add_special_tokens=True, tokenize=True)\n",
    "generation_text = llm.generate(chat, max_new_tokens=512, return_type='generation_text')\n",
    "generation_dict = llm.generate(chat, max_new_tokens=512, return_type='generation_dict')\n",
    "pred_logprobs = llm.generate(chat, max_new_tokens=512, return_type='pred_logprobs')\n",
    "raw = llm.generate(chat, max_new_tokens=512, return_type='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1:\n",
      "  Text: # Import necessary libraries \n",
      "\n",
      "  Tokens: ['#', 'Import', 'necessary', 'libraries', '\\n']\n",
      "  Token IDs: [29937, 16032, 5181, 9562, 13]\n",
      "  Starting Position: 0\n",
      "\n",
      "Line 2:\n",
      "  Text: import math \n",
      "\n",
      "  Tokens: ['import', 'math', '\\n']\n",
      "  Token IDs: [5215, 5844, 13]\n",
      "  Starting Position: 5\n",
      "\n",
      "Line 3:\n",
      "  Text: \n",
      "\n",
      "  Tokens: ['\\n']\n",
      "  Token IDs: [13]\n",
      "  Starting Position: 8\n",
      "\n",
      "Line 4:\n",
      "  Text: # Define function to check if two numbers are close to each other \n",
      "\n",
      "  Tokens: ['#', 'Define', 'function', 'to', 'check', 'if', 'two', 'numbers', 'are', 'close', 'to', 'each', 'other', '\\n']\n",
      "  Token IDs: [29937, 22402, 740, 304, 1423, 565, 1023, 3694, 526, 3802, 304, 1269, 916, 13]\n",
      "  Starting Position: 9\n",
      "\n",
      "Line 5:\n",
      "  Text: def has _ close _ elements ( numbers : List [ float ], threshold : float ) -> bool : \n",
      "\n",
      "  Tokens: ['def', 'has', '_', 'close', '_', 'elements', '(', 'numbers', ':', 'List', '[', 'float', '],', 'threshold', ':', 'float', ')', '->', 'bool', ':', '\\n']\n",
      "  Token IDs: [1753, 756, 29918, 5358, 29918, 17664, 29898, 20326, 29901, 2391, 29961, 7411, 1402, 16897, 29901, 5785, 29897, 1599, 6120, 29901, 13]\n",
      "  Starting Position: 23\n",
      "\n",
      "Line 6:\n",
      "  Text:    # Initialize variables \n",
      "\n",
      "  Tokens: ['  ', '#', 'Initialize', 'variables', '\\n']\n",
      "  Token IDs: [1678, 396, 25455, 3651, 13]\n",
      "  Starting Position: 44\n",
      "\n",
      "Line 7:\n",
      "  Text:    min _ distance = math . inf \n",
      "\n",
      "  Tokens: ['  ', 'min', '_', 'distance', '=', 'math', '.', 'inf', '\\n']\n",
      "  Token IDs: [1678, 1375, 29918, 19244, 353, 5844, 29889, 7192, 13]\n",
      "  Starting Position: 49\n",
      "\n",
      "Line 8:\n",
      "  Text:    min _ index = - 1 \n",
      "\n",
      "  Tokens: ['  ', 'min', '_', 'index', '=', '-', '1', '\\n']\n",
      "  Token IDs: [1678, 1375, 29918, 2248, 353, 448, 29896, 13]\n",
      "  Starting Position: 58\n",
      "\n",
      "Line 9:\n",
      "  Text:     \n",
      "\n",
      "  Tokens: ['   ', '\\n']\n",
      "  Token IDs: [268, 13]\n",
      "  Starting Position: 66\n",
      "\n",
      "Line 10:\n",
      "  Text:    # Loop through all numbers in the list \n",
      "\n",
      "  Tokens: ['  ', '#', 'Loop', 'through', 'all', 'numbers', 'in', 'the', 'list', '\\n']\n",
      "  Token IDs: [1678, 396, 21493, 1549, 599, 3694, 297, 278, 1051, 13]\n",
      "  Starting Position: 68\n",
      "\n",
      "Line 11:\n",
      "  Text:    for num in numbers : \n",
      "\n",
      "  Tokens: ['  ', 'for', 'num', 'in', 'numbers', ':', '\\n']\n",
      "  Token IDs: [1678, 363, 954, 297, 3694, 29901, 13]\n",
      "  Starting Position: 78\n",
      "\n",
      "Line 12:\n",
      "  Text:        # Calcul ate distance between num and all other numbers in the list \n",
      "\n",
      "  Tokens: ['      ', '#', 'Calcul', 'ate', 'distance', 'between', 'num', 'and', 'all', 'other', 'numbers', 'in', 'the', 'list', '\\n']\n",
      "  Token IDs: [4706, 396, 20535, 403, 5418, 1546, 954, 322, 599, 916, 3694, 297, 278, 1051, 13]\n",
      "  Starting Position: 85\n",
      "\n",
      "Line 13:\n",
      "  Text:        distance = math . sqrt ( sum (( x - num ) **  2 for x in numbers if x != num )) \n",
      "\n",
      "  Tokens: ['      ', 'distance', '=', 'math', '.', 'sqrt', '(', 'sum', '((', 'x', '-', 'num', ')', '**', '', '2', 'for', 'x', 'in', 'numbers', 'if', 'x', '!=', 'num', '))', '\\n']\n",
      "  Token IDs: [4706, 5418, 353, 5844, 29889, 3676, 29898, 2083, 3552, 29916, 448, 954, 29897, 3579, 29871, 29906, 363, 921, 297, 3694, 565, 921, 2804, 954, 876, 13]\n",
      "  Starting Position: 100\n",
      "\n",
      "Line 14:\n",
      "  Text:         \n",
      "\n",
      "  Tokens: ['       ', '\\n']\n",
      "  Token IDs: [308, 13]\n",
      "  Starting Position: 126\n",
      "\n",
      "Line 15:\n",
      "  Text:        # If distance is less than threshold , update min _ distance and min _ index \n",
      "\n",
      "  Tokens: ['      ', '#', 'If', 'distance', 'is', 'less', 'than', 'threshold', ',', 'update', 'min', '_', 'distance', 'and', 'min', '_', 'index', '\\n']\n",
      "  Token IDs: [4706, 396, 960, 5418, 338, 3109, 1135, 16897, 29892, 2767, 1375, 29918, 19244, 322, 1375, 29918, 2248, 13]\n",
      "  Starting Position: 128\n",
      "\n",
      "Line 16:\n",
      "  Text:        if distance < threshold : \n",
      "\n",
      "  Tokens: ['      ', 'if', 'distance', '<', 'threshold', ':', '\\n']\n",
      "  Token IDs: [4706, 565, 5418, 529, 16897, 29901, 13]\n",
      "  Starting Position: 146\n",
      "\n",
      "Line 17:\n",
      "  Text:            min _ distance = distance \n",
      "\n",
      "  Tokens: ['          ', 'min', '_', 'distance', '=', 'distance', '\\n']\n",
      "  Token IDs: [9651, 1375, 29918, 19244, 353, 5418, 13]\n",
      "  Starting Position: 153\n",
      "\n",
      "Line 18:\n",
      "  Text:            min _ index = i \n",
      "\n",
      "  Tokens: ['          ', 'min', '_', 'index', '=', 'i', '\\n']\n",
      "  Token IDs: [9651, 1375, 29918, 2248, 353, 474, 13]\n",
      "  Starting Position: 160\n",
      "\n",
      "Line 19:\n",
      "  Text:             \n",
      "\n",
      "  Tokens: ['           ', '\\n']\n",
      "  Token IDs: [632, 13]\n",
      "  Starting Position: 167\n",
      "\n",
      "Line 20:\n",
      "  Text:    # Return true if min _ distance is less than threshold and min _ index is not - 1 \n",
      "\n",
      "  Tokens: ['  ', '#', 'Return', 'true', 'if', 'min', '_', 'distance', 'is', 'less', 'than', 'threshold', 'and', 'min', '_', 'index', 'is', 'not', '-', '1', '\\n']\n",
      "  Token IDs: [1678, 396, 7106, 1565, 565, 1375, 29918, 19244, 338, 3109, 1135, 16897, 322, 1375, 29918, 2248, 338, 451, 448, 29896, 13]\n",
      "  Starting Position: 169\n",
      "\n",
      "Line 21:\n",
      "  Text:    return min _ distance < threshold and min _ index != - 1 \n",
      "\n",
      "  Tokens: ['  ', 'return', 'min', '_', 'distance', '<', 'threshold', 'and', 'min', '_', 'index', '!=', '-', '1', '\\n']\n",
      "  Token IDs: [1678, 736, 1375, 29918, 19244, 529, 16897, 322, 1375, 29918, 2248, 2804, 448, 29896, 13]\n",
      "  Starting Position: 190\n",
      "\n",
      "Line 22:\n",
      "  Text: \n",
      "\n",
      "  Tokens: ['\\n']\n",
      "  Token IDs: [13]\n",
      "  Starting Position: 205\n",
      "\n",
      "Line 23:\n",
      "  Text: # Example usage \n",
      "\n",
      "  Tokens: ['#', 'Example', 'usage', '\\n']\n",
      "  Token IDs: [29937, 8741, 8744, 13]\n",
      "  Starting Position: 206\n",
      "\n",
      "Line 24:\n",
      "  Text: numbers = [ 1 . 0 ,  2 . 0 ,  3 . 0 ] \n",
      "\n",
      "  Tokens: ['numbers', '=', '[', '1', '.', '0', ',', '', '2', '.', '0', ',', '', '3', '.', '0', ']', '\\n']\n",
      "  Token IDs: [20326, 353, 518, 29896, 29889, 29900, 29892, 29871, 29906, 29889, 29900, 29892, 29871, 29941, 29889, 29900, 29962, 13]\n",
      "  Starting Position: 210\n",
      "\n",
      "Line 25:\n",
      "  Text: th reshold =  0 . 5 \n",
      "\n",
      "  Tokens: ['th', 'reshold', '=', '', '0', '.', '5', '\\n']\n",
      "  Token IDs: [386, 12268, 353, 29871, 29900, 29889, 29945, 13]\n",
      "  Starting Position: 228\n",
      "\n",
      "Line 26:\n",
      "  Text: has _ close _ elements ( numbers , threshold )  # Returns False \n",
      "\n",
      "  Tokens: ['has', '_', 'close', '_', 'elements', '(', 'numbers', ',', 'threshold', ')', '', '#', 'Returns', 'False', '\\n']\n",
      "  Token IDs: [5349, 29918, 5358, 29918, 17664, 29898, 20326, 29892, 16897, 29897, 29871, 396, 16969, 7700, 13]\n",
      "  Starting Position: 236\n",
      "\n",
      "Line 27:\n",
      "  Text: has _ close _ elements ( numbers , threshold )  # Returns True \n",
      "\n",
      "  Tokens: ['has', '_', 'close', '_', 'elements', '(', 'numbers', ',', 'threshold', ')', '', '#', 'Returns', 'True', '\\n']\n",
      "  Token IDs: [5349, 29918, 5358, 29918, 17664, 29898, 20326, 29892, 16897, 29897, 29871, 396, 16969, 5852, 13]\n",
      "  Starting Position: 251\n",
      "\n",
      "Line 28:\n",
      "  Text: \n",
      "\n",
      "  Tokens: ['\\n']\n",
      "  Token IDs: [13]\n",
      "  Starting Position: 266\n",
      "\n",
      "Line 29:\n",
      "  Text: # Example usage with custom threshold \n",
      "\n",
      "  Tokens: ['#', 'Example', 'usage', 'with', 'custom', 'threshold', '\\n']\n",
      "  Token IDs: [29937, 8741, 8744, 411, 2888, 16897, 13]\n",
      "  Starting Position: 267\n",
      "\n",
      "Line 30:\n",
      "  Text: numbers = [ 1 . 0 ,  2 . 8 ,  3 . 0 ,  4 . 0 ,  5 . 0 ,  2 . 0 ] \n",
      "\n",
      "  Tokens: ['numbers', '=', '[', '1', '.', '0', ',', '', '2', '.', '8', ',', '', '3', '.', '0', ',', '', '4', '.', '0', ',', '', '5', '.', '0', ',', '', '2', '.', '0', ']', '\\n']\n",
      "  Token IDs: [20326, 353, 518, 29896, 29889, 29900, 29892, 29871, 29906, 29889, 29947, 29892, 29871, 29941, 29889, 29900, 29892, 29871, 29946, 29889, 29900, 29892, 29871, 29945, 29889, 29900, 29892, 29871, 29906, 29889, 29900, 29962, 13]\n",
      "  Starting Position: 274\n",
      "\n",
      "Line 31:\n",
      "  Text: th reshold =  0 . 3 \n",
      "\n",
      "  Tokens: ['th', 'reshold', '=', '', '0', '.', '3', '\\n']\n",
      "  Token IDs: [386, 12268, 353, 29871, 29900, 29889, 29941, 13]\n",
      "  Starting Position: 307\n",
      "\n",
      "Line 32:\n",
      "  Text: has _ close _ elements ( numbers , threshold )  # Returns True </s>\n",
      "  Tokens: ['has', '_', 'close', '_', 'elements', '(', 'numbers', ',', 'threshold', ')', '', '#', 'Returns', 'True', '</s>']\n",
      "  Token IDs: [5349, 29918, 5358, 29918, 17664, 29898, 20326, 29892, 16897, 29897, 29871, 396, 16969, 5852, 2]\n",
      "  Starting Position: 315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_lines(token_ids):\n",
    "    token_split = []\n",
    "    id_split = []\n",
    "    position_split = []\n",
    "    cur_token_split = []\n",
    "    cur_id_split = []\n",
    "    cur_position = 0\n",
    "\n",
    "    tokens = [llm.tokenizer.decode(id) for id in token_ids]\n",
    "\n",
    "    for token, token_id in zip(tokens, token_ids):\n",
    "        cur_token_split.append(token)\n",
    "        cur_id_split.append(token_id)\n",
    "        if '\\n' in token:\n",
    "            token_split.append(cur_token_split)\n",
    "            id_split.append(cur_id_split)\n",
    "            position_split.append(cur_position)\n",
    "            cur_token_split = []\n",
    "            cur_id_split = []\n",
    "            cur_position = len(token_split[-1]) + cur_position\n",
    "\n",
    "    # Append any remaining tokens that did not end with a newline\n",
    "    if cur_token_split:\n",
    "        token_split.append(cur_token_split)\n",
    "        id_split.append(cur_id_split)\n",
    "        position_split.append(cur_position)\n",
    "\n",
    "    # Create list of lines by joining tokens\n",
    "    line_split = [' '.join(split) for split in token_split]\n",
    "\n",
    "    return line_split, token_split, id_split, position_split\n",
    "\n",
    "tokens = generation_dict['new_text']\n",
    "token_ids = generation_dict['new_ids']\n",
    "\n",
    "line_split, token_split, id_split, position_split = split_lines(token_ids)\n",
    "\n",
    "for i, (line, tokens, ids, pos) in enumerate(zip(line_split, token_split, id_split, position_split), 1):\n",
    "    print(f\"Line {i}:\")\n",
    "    print(f\"  Text: {line}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Token IDs: {ids}\")\n",
    "    print(f\"  Starting Position: {pos}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_indices_split [[0, 1, 2, 3, 4], [5, 6, 7], [8], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], [44, 45, 46, 47, 48], [49, 50, 51, 52, 53, 54, 55, 56, 57], [58, 59, 60, 61, 62, 63, 64, 65], [66, 67], [68, 69, 70, 71, 72, 73, 74, 75, 76, 77], [78, 79, 80, 81, 82, 83, 84], [85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125], [126, 127], [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], [146, 147, 148, 149, 150, 151, 152], [153, 154, 155, 156, 157, 158, 159], [160, 161, 162, 163, 164, 165, 166], [167, 168], [169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189], [190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204], [205], [206, 207, 208, 209], [210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227], [228, 229, 230, 231, 232, 233, 234, 235], [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], [251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265], [266], [267, 268, 269, 270, 271, 272, 273], [274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306], [307, 308, 309, 310, 311, 312, 313, 314], [315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329]]\n"
     ]
    }
   ],
   "source": [
    "def token_indices_by_line(token_ids):\n",
    "    line_split, token_split,_,_ = split_lines(token_ids)\n",
    "    token_indices_split =[]\n",
    "    for i, split in enumerate(token_split):\n",
    "        # print(token_split[i])\n",
    "        start_index = sum(len(token_split[j]) for j in range(i))\n",
    "        token_indices = [k for k in range(start_index, start_index + len(split))]\n",
    "        token_indices_split.append(token_indices)\n",
    "    return token_indices_split\n",
    "\n",
    "tokens = generation_dict['new_text']\n",
    "token_ids = generation_dict['new_ids']\n",
    "token_indices_split = token_indices_by_line(token_ids)\n",
    "print('token_indices_split', token_indices_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty_dict {'MTE': 0.45145514607429504}\n",
      "uncertainty_dict [('# Import necessary libraries \\n', {'MTE': 1.8605102}), ('import math \\n', {'MTE': 0.60075325}), ('\\n', {'MTE': 1.2995349}), ('# Define function to check if two numbers are close to each other \\n', {'MTE': 0.98222786}), ('def has _ close _ elements ( numbers : List [ float ], threshold : float ) -> bool : \\n', {'MTE': 0.030683257}), ('   # Initialize variables \\n', {'MTE': 1.6680057}), ('   min _ distance = math . inf \\n', {'MTE': 0.7685098}), ('   min _ index = - 1 \\n', {'MTE': 0.6230825}), ('    \\n', {'MTE': 0.5389683}), ('   # Loop through all numbers in the list \\n', {'MTE': 0.73365116}), ('   for num in numbers : \\n', {'MTE': 0.262511}), ('       # Calcul ate distance between num and all other numbers in the list \\n', {'MTE': 0.902841}), ('       distance = math . sqrt ( sum (( x - num ) **  2 for x in numbers if x != num )) \\n', {'MTE': 0.45249292}), ('        \\n', {'MTE': 0.4203303}), ('       # If distance is less than threshold , update min _ distance and min _ index \\n', {'MTE': 0.62540406}), ('       if distance < threshold : \\n', {'MTE': 0.08351634}), ('           min _ distance = distance \\n', {'MTE': 0.061789993}), ('           min _ index = i \\n', {'MTE': 0.44464806}), ('            \\n', {'MTE': 0.61617535}), ('   # Return true if min _ distance is less than threshold and min _ index is not - 1 \\n', {'MTE': 0.7655078}), ('   return min _ distance < threshold and min _ index != - 1 \\n', {'MTE': 0.11154909}), ('# Example usage \\n', {'MTE': 0.77779114}), ('numbers = [ 1 . 0 ,  2 . 0 ,  3 . 0 ] \\n', {'MTE': 0.14164175}), ('th reshold =  0 . 5 \\n', {'MTE': 0.08969442}), ('has _ close _ elements ( numbers , threshold )  # Returns False \\n', {'MTE': 0.44289526}), ('has _ close _ elements ( numbers , threshold )  # Returns True \\n', {'MTE': 0.103615396}), ('# Example usage with custom threshold \\n', {'MTE': 1.6292562}), ('numbers = [ 1 . 0 ,  2 . 8 ,  3 . 0 ,  4 . 0 ,  5 . 0 ,  2 . 0 ] \\n', {'MTE': 0.07311078}), ('th reshold =  0 . 3 \\n', {'MTE': 0.05427062}), ('has _ close _ elements ( numbers , threshold )  # Returns True </s>', {'MTE': 0.074630834})]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mte(uncertainty_dict, pred_logprobs):\n",
    "\n",
    "    new_ids = uncertainty_dict['new_ids']\n",
    "    new_text = uncertainty_dict['new_text']\n",
    "\n",
    "    # Compute the entropy for each tensor in the tuple\n",
    "    entropy_list = []\n",
    "    for log_prob_tensor in pred_logprobs:\n",
    "        # Convert log probabilities to probabilities\n",
    "        probabilities = torch.exp(log_prob_tensor)\n",
    "        # Compute entropy using the probabilities and their log probabilities\n",
    "        entropy = -(probabilities * log_prob_tensor).sum(dim=1)\n",
    "        entropy_list.append(entropy)    \n",
    "\n",
    "    \n",
    "    all_entropies = torch.cat(entropy_list)\n",
    "    global_mte = all_entropies.mean().item()\n",
    "\n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'MTE': global_mte})\n",
    "\n",
    "    # Split the text into lines\n",
    "    line_split, token_split,_,_ = split_lines(new_ids)\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "\n",
    "    if 'line_unc_scores' not in uncertainty_dict:\n",
    "        uncertainty_dict['line_unc_scores'] = []\n",
    "    \n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "    # for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        # line_tokens = [new_text[i] for i in line_indices]\n",
    "        # line_text = ''.join(line_tokens)\n",
    "        line_text = line_split[i]\n",
    "        token_entropies_line = []\n",
    "\n",
    "        for index in line_indices:\n",
    "            token_entropies_line.append(all_entropies[index])\n",
    "        mte_score = np.mean(token_entropies_line)\n",
    "        new_scores = {\"MTE\": mte_score}\n",
    "        \n",
    "        # Update or append line scores\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "    \n",
    "    return uncertainty_dict\n",
    "\n",
    "uncertainty_dict = generation_dict.copy()\n",
    "uncertainty_dict = mte(uncertainty_dict, pred_logprobs)\n",
    "print('uncertainty_dict', uncertainty_dict['global_unc_score'])\n",
    "print('uncertainty_dict', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty_dict {'MTE': 0.45145514607429504, 'MSP': 1.0}\n",
      "uncertainty_dict [('# Import necessary libraries \\n', {'MTE': 1.8605102, 'MSP': 0.9631931421175698}), ('import math \\n', {'MTE': 0.60075325, 'MSP': 0.4311580859271109}), ('\\n', {'MTE': 1.2995349, 'MSP': 0.44248056411743164}), ('# Define function to check if two numbers are close to each other \\n', {'MTE': 0.98222786, 'MSP': 0.9931773175858839}), ('def has _ close _ elements ( numbers : List [ float ], threshold : float ) -> bool : \\n', {'MTE': 0.030683257, 'MSP': 0.10625167046089246}), ('   # Initialize variables \\n', {'MTE': 1.6680057, 'MSP': 0.9774341491921095}), ('   min _ distance = math . inf \\n', {'MTE': 0.7685098, 'MSP': 0.9306484062996937}), ('   min _ index = - 1 \\n', {'MTE': 0.6230825, 'MSP': 0.9014650450721394}), ('    \\n', {'MTE': 0.5389683, 'MSP': 0.5678201430834875}), ('   # Loop through all numbers in the list \\n', {'MTE': 0.73365116, 'MSP': 0.955953279146527}), ('   for num in numbers : \\n', {'MTE': 0.262511, 'MSP': 0.6720766286807789}), ('       # Calcul ate distance between num and all other numbers in the list \\n', {'MTE': 0.902841, 'MSP': 0.9952606344058286}), ('       distance = math . sqrt ( sum (( x - num ) **  2 for x in numbers if x != num )) \\n', {'MTE': 0.45249292, 'MSP': 0.9960632240520789}), ('        \\n', {'MTE': 0.4203303, 'MSP': 0.2965663424094629}), ('       # If distance is less than threshold , update min _ distance and min _ index \\n', {'MTE': 0.62540406, 'MSP': 0.981271637217056}), ('       if distance < threshold : \\n', {'MTE': 0.08351634, 'MSP': 0.13214101361628483}), ('           min _ distance = distance \\n', {'MTE': 0.061789993, 'MSP': 0.07673889607615969}), ('           min _ index = i \\n', {'MTE': 0.44464806, 'MSP': 0.759960049145757}), ('            \\n', {'MTE': 0.61617535, 'MSP': 0.4221199370259967}), ('   # Return true if min _ distance is less than threshold and min _ index is not - 1 \\n', {'MTE': 0.7655078, 'MSP': 0.997942978276272}), ('   return min _ distance < threshold and min _ index != - 1 \\n', {'MTE': 0.11154909, 'MSP': 0.31602942952222335}), ('# Example usage \\n', {'MTE': 0.77779114, 'MSP': 0.7047582628044351}), ('numbers = [ 1 . 0 ,  2 . 0 ,  3 . 0 ] \\n', {'MTE': 0.14164175, 'MSP': 0.631097469918336}), ('th reshold =  0 . 5 \\n', {'MTE': 0.08969442, 'MSP': 0.14226605307588425}), ('has _ close _ elements ( numbers , threshold )  # Returns False \\n', {'MTE': 0.44289526, 'MSP': 0.932952851686476}), ('has _ close _ elements ( numbers , threshold )  # Returns True \\n', {'MTE': 0.103615396, 'MSP': 0.35931870291233436}), ('# Example usage with custom threshold \\n', {'MTE': 1.6292562, 'MSP': 0.9850661215746125}), ('numbers = [ 1 . 0 ,  2 . 8 ,  3 . 0 ,  4 . 0 ,  5 . 0 ,  2 . 0 ] \\n', {'MTE': 0.07311078, 'MSP': 0.5092513715891083}), ('th reshold =  0 . 3 \\n', {'MTE': 0.05427062, 'MSP': 0.06338842240208853}), ('has _ close _ elements ( numbers , threshold )  # Returns True </s>', {'MTE': 0.074630834, 'MSP': 0.4520144812470346})]\n"
     ]
    }
   ],
   "source": [
    "def msp(uncertainty_dict, pred_logprobs):\n",
    "\n",
    "    new_ids = uncertainty_dict['new_ids']\n",
    "    new_text = uncertainty_dict['new_text']\n",
    "\n",
    "    # Compute the probabilities for each tensor in the tuple\n",
    "    prob_list = []\n",
    "    for i, log_prob_tensor in enumerate(pred_logprobs):\n",
    "        # Convert log probabilities to probabilities\n",
    "        probabilities = torch.exp(log_prob_tensor)\n",
    "        index = new_ids[i]\n",
    "        prob_list.append(probabilities[0, index].item())    \n",
    "\n",
    "    # Global MSP score\n",
    "    total_prob = 1.0\n",
    "    for prob in prob_list:\n",
    "        total_prob *= prob\n",
    "    global_msp = 1 - total_prob\n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'MSP': global_msp})\n",
    "\n",
    "    # Linewise MSP scores\n",
    "    # Split the text into lines\n",
    "    line_split, token_split,_,_ = split_lines(new_ids)\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "\n",
    "    if 'line_unc_scores' not in uncertainty_dict:\n",
    "        uncertainty_dict['line_unc_scores'] = []\n",
    "    \n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        line_text = line_split[i]\n",
    "        line_prob = 1.0\n",
    "        for index in line_indices:\n",
    "            line_prob *= prob_list[index]\n",
    "        line_msp = 1 - line_prob\n",
    "        new_scores = {\"MSP\": line_msp}\n",
    "\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "\n",
    "    return uncertainty_dict\n",
    "\n",
    "\n",
    "uncertainty_dict = msp(uncertainty_dict, pred_logprobs)\n",
    "print('uncertainty_dict', uncertainty_dict['global_unc_score'])\n",
    "print('uncertainty_dict', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_unc_score {'MTE': 0.45145514607429504, 'MSP': 1.0, 'PP': 1.1318415078826611}\n",
      "line_unc_scores [('# Import necessary libraries \\n', {'MTE': 1.8605102, 'MSP': 0.9631931421175698, 'PP': 1.5805363668121513}), ('import math \\n', {'MTE': 0.60075325, 'MSP': 0.4311580859271109, 'PP': 1.1392235816622556}), ('\\n', {'MTE': 1.2995349, 'MSP': 0.44248056411743164, 'PP': 1.4992675916039133}), ('# Define function to check if two numbers are close to each other \\n', {'MTE': 0.98222786, 'MSP': 0.9931773175858839, 'PP': 1.2800943784403396}), ('def has _ close _ elements ( numbers : List [ float ], threshold : float ) -> bool : \\n', {'MTE': 0.030683257, 'MSP': 0.10625167046089246, 'PP': 1.003714593403087}), ('   # Initialize variables \\n', {'MTE': 1.6680057, 'MSP': 0.9774341491921095, 'PP': 1.6914534896269529}), ('   min _ distance = math . inf \\n', {'MTE': 0.7685098, 'MSP': 0.9306484062996937, 'PP': 1.2281675053440744}), ('   min _ index = - 1 \\n', {'MTE': 0.6230825, 'MSP': 0.9014650450721394, 'PP': 1.2223589425744608}), ('    \\n', {'MTE': 0.5389683, 'MSP': 0.5678201430834875, 'PP': 1.3374238224472081}), ('   # Loop through all numbers in the list \\n', {'MTE': 0.73365116, 'MSP': 0.955953279146527, 'PP': 1.2416430114666823}), ('   for num in numbers : \\n', {'MTE': 0.262511, 'MSP': 0.6720766286807789, 'PP': 1.116731362652911}), ('       # Calcul ate distance between num and all other numbers in the list \\n', {'MTE': 0.902841, 'MSP': 0.9952606344058286, 'PP': 1.2805735626208525}), ('       distance = math . sqrt ( sum (( x - num ) **  2 for x in numbers if x != num )) \\n', {'MTE': 0.45249292, 'MSP': 0.9960632240520789, 'PP': 1.159077196735435}), ('        \\n', {'MTE': 0.4203303, 'MSP': 0.2965663424094629, 'PP': 1.1296617546154104}), ('       # If distance is less than threshold , update min _ distance and min _ index \\n', {'MTE': 0.62540406, 'MSP': 0.981271637217056, 'PP': 1.165528465698248}), ('       if distance < threshold : \\n', {'MTE': 0.08351634, 'MSP': 0.13214101361628483, 'PP': 1.0141327920597838}), ('           min _ distance = distance \\n', {'MTE': 0.061789993, 'MSP': 0.07673889607615969, 'PP': 1.0079374945068174}), ('           min _ index = i \\n', {'MTE': 0.44464806, 'MSP': 0.759960049145757, 'PP': 1.1517678703720278}), ('            \\n', {'MTE': 0.61617535, 'MSP': 0.4221199370259967, 'PP': 1.20931868410155}), ('   # Return true if min _ distance is less than threshold and min _ index is not - 1 \\n', {'MTE': 0.7655078, 'MSP': 0.997942978276272, 'PP': 1.2265406605215505}), ('   return min _ distance < threshold and min _ index != - 1 \\n', {'MTE': 0.11154909, 'MSP': 0.31602942952222335, 'PP': 1.0177072946963912}), ('# Example usage \\n', {'MTE': 0.77779114, 'MSP': 0.7047582628044351, 'PP': 1.2354102398478206}), ('numbers = [ 1 . 0 ,  2 . 0 ,  3 . 0 ] \\n', {'MTE': 0.14164175, 'MSP': 0.631097469918336, 'PP': 1.0391480921385106}), ('th reshold =  0 . 5 \\n', {'MTE': 0.08969442, 'MSP': 0.14226605307588425, 'PP': 1.0133852023604961}), ('has _ close _ elements ( numbers , threshold )  # Returns False \\n', {'MTE': 0.44289526, 'MSP': 0.932952851686476, 'PP': 1.133007395274621}), ('has _ close _ elements ( numbers , threshold )  # Returns True \\n', {'MTE': 0.103615396, 'MSP': 0.35931870291233436, 'PP': 1.020786776870926}), ('# Example usage with custom threshold \\n', {'MTE': 1.6292562, 'MSP': 0.9850661215746125, 'PP': 1.5163354933700373}), ('numbers = [ 1 . 0 ,  2 . 8 ,  3 . 0 ,  4 . 0 ,  5 . 0 ,  2 . 0 ] \\n', {'MTE': 0.07311078, 'MSP': 0.5092513715891083, 'PP': 1.0150637967476452}), ('th reshold =  0 . 3 \\n', {'MTE': 0.05427062, 'MSP': 0.06338842240208853, 'PP': 1.0056901196127577}), ('has _ close _ elements ( numbers , threshold )  # Returns True </s>', {'MTE': 0.074630834, 'MSP': 0.4520144812470346, 'PP': 1.0281853938559466})]\n"
     ]
    }
   ],
   "source": [
    "def pp(uncertainty_dict, pred_logprobs):\n",
    "\n",
    "    new_ids = uncertainty_dict['new_ids']\n",
    "    new_text = uncertainty_dict['new_text']\n",
    "\n",
    "    log_prob_list = []\n",
    "    for i, log_prob_tensor in enumerate(pred_logprobs):\n",
    "        index = new_ids[i]\n",
    "        log_prob_list.append(pred_logprobs[i][0, index].item())\n",
    "\n",
    "    # Global PP score\n",
    "    avg_log_prob = sum(log_prob_list) / len(log_prob_list)\n",
    "    global_pp = 2 ** (-avg_log_prob)\n",
    "\n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'PP': global_pp})\n",
    "\n",
    "    # Linewise PP scores\n",
    "    # Split the text into lines\n",
    "    line_split, token_split,_,_ = split_lines(new_ids)\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "\n",
    "    if 'line_unc_scores' not in uncertainty_dict:\n",
    "        uncertainty_dict['line_unc_scores'] = []\n",
    "    \n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_text = line_split[i]\n",
    "        line_logprobs = [log_prob_list[i] for i in line_indices]\n",
    "        avg_line_log_prob = sum(line_logprobs) / len(line_logprobs)\n",
    "        line_pp =  2 ** (-avg_line_log_prob)\n",
    "        new_scores = {\"PP\": line_pp}\n",
    "\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "\n",
    "    return uncertainty_dict\n",
    "\n",
    "\n",
    "uncertainty_dict = pp(uncertainty_dict, pred_logprobs)\n",
    "print('global_unc_score', uncertainty_dict['global_unc_score'])\n",
    "print('line_unc_scores', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_unc_score {'MTE': 0.45145514607429504, 'MSP': 1.0, 'PP': 1.1318415078826611, 'pTrue': -10.095816612243652}\n",
      "line_unc_scores [('# Import necessary libraries \\n', {'MTE': 1.8605102, 'MSP': 0.9631931421175698, 'PP': 1.5805363668121513, 'pTrue': -10.263795852661133}), ('import math \\n', {'MTE': 0.60075325, 'MSP': 0.4311580859271109, 'PP': 1.1392235816622556, 'pTrue': -10.534920692443848}), ('\\n', {'MTE': 1.2995349, 'MSP': 0.44248056411743164, 'PP': 1.4992675916039133, 'pTrue': -9.946125030517578}), ('# Define function to check if two numbers are close to each other \\n', {'MTE': 0.98222786, 'MSP': 0.9931773175858839, 'PP': 1.2800943784403396, 'pTrue': -10.49610424041748}), ('def has _ close _ elements ( numbers : List [ float ], threshold : float ) -> bool : \\n', {'MTE': 0.030683257, 'MSP': 0.10625167046089246, 'PP': 1.003714593403087, 'pTrue': -9.940467834472656}), ('   # Initialize variables \\n', {'MTE': 1.6680057, 'MSP': 0.9774341491921095, 'PP': 1.6914534896269529, 'pTrue': -10.314358711242676}), ('   min _ distance = math . inf \\n', {'MTE': 0.7685098, 'MSP': 0.9306484062996937, 'PP': 1.2281675053440744, 'pTrue': -10.216777801513672}), ('   min _ index = - 1 \\n', {'MTE': 0.6230825, 'MSP': 0.9014650450721394, 'PP': 1.2223589425744608, 'pTrue': -10.24329948425293}), ('    \\n', {'MTE': 0.5389683, 'MSP': 0.5678201430834875, 'PP': 1.3374238224472081, 'pTrue': -9.96009635925293}), ('   # Loop through all numbers in the list \\n', {'MTE': 0.73365116, 'MSP': 0.955953279146527, 'PP': 1.2416430114666823, 'pTrue': -10.504659652709961}), ('   for num in numbers : \\n', {'MTE': 0.262511, 'MSP': 0.6720766286807789, 'PP': 1.116731362652911, 'pTrue': -10.233217239379883}), ('       # Calcul ate distance between num and all other numbers in the list \\n', {'MTE': 0.902841, 'MSP': 0.9952606344058286, 'PP': 1.2805735626208525, 'pTrue': -10.52171802520752}), ('       distance = math . sqrt ( sum (( x - num ) **  2 for x in numbers if x != num )) \\n', {'MTE': 0.45249292, 'MSP': 0.9960632240520789, 'PP': 1.159077196735435, 'pTrue': -10.411134719848633}), ('        \\n', {'MTE': 0.4203303, 'MSP': 0.2965663424094629, 'PP': 1.1296617546154104, 'pTrue': -9.951554298400879}), ('       # If distance is less than threshold , update min _ distance and min _ index \\n', {'MTE': 0.62540406, 'MSP': 0.981271637217056, 'PP': 1.165528465698248, 'pTrue': -10.354822158813477}), ('       if distance < threshold : \\n', {'MTE': 0.08351634, 'MSP': 0.13214101361628483, 'PP': 1.0141327920597838, 'pTrue': -10.033365249633789}), ('           min _ distance = distance \\n', {'MTE': 0.061789993, 'MSP': 0.07673889607615969, 'PP': 1.0079374945068174, 'pTrue': -10.498404502868652}), ('           min _ index = i \\n', {'MTE': 0.44464806, 'MSP': 0.759960049145757, 'PP': 1.1517678703720278, 'pTrue': -10.457489013671875}), ('            \\n', {'MTE': 0.61617535, 'MSP': 0.4221199370259967, 'PP': 1.20931868410155, 'pTrue': -9.968791007995605}), ('   # Return true if min _ distance is less than threshold and min _ index is not - 1 \\n', {'MTE': 0.7655078, 'MSP': 0.997942978276272, 'PP': 1.2265406605215505, 'pTrue': -10.069470405578613}), ('   return min _ distance < threshold and min _ index != - 1 \\n', {'MTE': 0.11154909, 'MSP': 0.31602942952222335, 'PP': 1.0177072946963912, 'pTrue': -10.193085670471191}), ('# Example usage \\n', {'MTE': 0.77779114, 'MSP': 0.7047582628044351, 'PP': 1.2354102398478206, 'pTrue': -9.986577987670898}), ('numbers = [ 1 . 0 ,  2 . 0 ,  3 . 0 ] \\n', {'MTE': 0.14164175, 'MSP': 0.631097469918336, 'PP': 1.0391480921385106, 'pTrue': -10.42590618133545}), ('th reshold =  0 . 5 \\n', {'MTE': 0.08969442, 'MSP': 0.14226605307588425, 'PP': 1.0133852023604961, 'pTrue': -10.278485298156738}), ('has _ close _ elements ( numbers , threshold )  # Returns False \\n', {'MTE': 0.44289526, 'MSP': 0.932952851686476, 'PP': 1.133007395274621, 'pTrue': -6.1372270584106445}), ('has _ close _ elements ( numbers , threshold )  # Returns True \\n', {'MTE': 0.103615396, 'MSP': 0.35931870291233436, 'PP': 1.020786776870926, 'pTrue': -0.004365557339042425}), ('# Example usage with custom threshold \\n', {'MTE': 1.6292562, 'MSP': 0.9850661215746125, 'PP': 1.5163354933700373, 'pTrue': -9.86133098602295}), ('numbers = [ 1 . 0 ,  2 . 8 ,  3 . 0 ,  4 . 0 ,  5 . 0 ,  2 . 0 ] \\n', {'MTE': 0.07311078, 'MSP': 0.5092513715891083, 'PP': 1.0150637967476452, 'pTrue': -10.391526222229004}), ('th reshold =  0 . 3 \\n', {'MTE': 0.05427062, 'MSP': 0.06338842240208853, 'PP': 1.0056901196127577, 'pTrue': -10.24876594543457}), ('has _ close _ elements ( numbers , threshold )  # Returns True </s>', {'MTE': 0.074630834, 'MSP': 0.4520144812470346, 'PP': 1.0281853938559466, 'pTrue': -0.004050623159855604})]\n"
     ]
    }
   ],
   "source": [
    "def generate_pTrue_prompt(base_prompt: str):\n",
    "    system_message = \"\"\"Answer the following question. Answer True or False without explanation:\\n\"\"\"\n",
    "    user_message = f\"{base_prompt}\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\":user_message}\n",
    "        ]\n",
    "    return chat\n",
    "\n",
    "\n",
    "def extract_ptrue_score(generation_dict, pred_logprobs):\n",
    "    # Find the token for True or False\n",
    "    ptrue = 0.0\n",
    "    found_token = False\n",
    "    for index, (token_id) in enumerate(generation_dict['new_ids']):\n",
    "        token = llm.tokenizer.decode(token_id)\n",
    "        if 'True' in token:\n",
    "            ptrue = pred_logprobs[index][0, token_id].item()\n",
    "            found_token = True\n",
    "            break\n",
    "        elif 'False' in token:\n",
    "            true_token_id = llm.tokenizer('True')['input_ids'][1]\n",
    "            ptrue = pred_logprobs[index][0, true_token_id].item()\n",
    "            found_token = True\n",
    "            break\n",
    "    \n",
    "    # If no True or False token is found, use the first token's probability to be True\n",
    "    if not found_token:\n",
    "        true_token_id = llm.tokenizer('True')['input_ids'][1]\n",
    "        ptrue = pred_logprobs[0][0, true_token_id].item()\n",
    "    return ptrue\n",
    "\n",
    "def ptrue(uncertainty_dict, pred_logprobs):\n",
    "\n",
    "    input_text = uncertainty_dict['input_text']\n",
    "    new_text = uncertainty_dict['new_text']\n",
    "    new_ids = generation_dict['new_ids']\n",
    "\n",
    "    verification_prompt = f\"Question: {input_text}\\nProposed Answer: {new_text}\\nIs the content in the proposed answer correct?\\n(A) True\\n(B) False\\nAnswer True or False without explanation:\"\n",
    "    verification_prompt = generate_pTrue_prompt(verification_prompt)\n",
    "\n",
    "    global_ptrue_generation_dict = llm.generate(verification_prompt, max_new_tokens=512, return_type='generation_dict')\n",
    "    global_ptrue_pred_logprobs = llm.generate(verification_prompt, max_new_tokens=512, return_type='pred_logprobs')\n",
    "\n",
    "    global_ptrue = extract_ptrue_score(global_ptrue_generation_dict, global_ptrue_pred_logprobs)\n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'pTrue': global_ptrue})\n",
    "\n",
    "    # Linewise ptrue scores\n",
    "    # Split the text into lines\n",
    "    line_split, token_split,_,_ = split_lines(new_ids)\n",
    "\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_text = line_split[i]\n",
    "\n",
    "        line_verif_prompt = f\"Question: {input_text}\\nProposed Answer: {new_text}\\nIs the sentence \\\"{line_text}\\\" in the proposed answer correct?\\n(A) True\\n(B) False\\nAnswer True or False without explanation:\"\n",
    "        line_verif_prompt = generate_pTrue_prompt(line_verif_prompt)\n",
    "\n",
    "        line_ptrue_generation_dict = llm.generate(line_verif_prompt, max_new_tokens=512, return_type='generation_dict')\n",
    "        line_ptrue_pred_logprobs = llm.generate(line_verif_prompt, max_new_tokens=512, return_type='pred_logprobs')\n",
    "\n",
    "        line_ptrue = extract_ptrue_score(line_ptrue_generation_dict, line_ptrue_pred_logprobs)\n",
    "        new_scores = {\"pTrue\": line_ptrue}\n",
    "\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "\n",
    "    return uncertainty_dict\n",
    "\n",
    "uncertainty_dict = ptrue(uncertainty_dict, pred_logprobs)\n",
    "print('global_unc_score', uncertainty_dict['global_unc_score'])\n",
    "print('line_unc_scores', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line_text # Import necessary libraries \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxl220096/anaconda3/envs/llmuq/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line_text import math \n",
      "\n",
      "line_text \n",
      "\n",
      "line_text # Define function to check if two numbers are close to each other \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_mcse_prompt(input_text, line_responce):\n",
    "    system_message = \"\"\"Generate a response to the following input:\\n\"\"\"\n",
    "    base_prompt = input_text + '\\n' + line_responce\n",
    "    user_message = f\"{base_prompt}\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\":user_message}\n",
    "        ]\n",
    "    return chat\n",
    "\n",
    "def mcse(uncertainty_dict, pred_logprobs):\n",
    "    \n",
    "    input_text = uncertainty_dict['input_text']\n",
    "    new_ids = uncertainty_dict['new_ids']\n",
    "\n",
    "    num_samples = 3\n",
    "    total_logprob = 0\n",
    "    for _ in range(num_samples):\n",
    "        pred_logprobs = llm.generate(input_text, max_new_tokens=512, return_type='pred_logprobs')\n",
    "\n",
    "        log_prob_list = []\n",
    "        for i, log_prob_tensor in enumerate(pred_logprobs):\n",
    "            index = token_ids[i]\n",
    "            log_prob_list.append(pred_logprobs[i][0, index].item())\n",
    "        \n",
    "        temp_logprob = sum(log_prob_list)\n",
    "        total_logprob += temp_logprob\n",
    "    \n",
    "    global_mcse = -total_logprob / num_samples\n",
    "        \n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'MCSE': global_mcse})\n",
    "\n",
    "    # Linewise ptrue scores\n",
    "    # Split the text into lines\n",
    "    line_split, _,_,_ = split_lines(new_ids)\n",
    "\n",
    "    line_responce = ''\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_text = line_split[i]\n",
    "\n",
    "        print('line_text', line_text)\n",
    "\n",
    "        temp_mcse_prompt = generate_mcse_prompt(input_text, line_responce)\n",
    "\n",
    "        line_total_logprob = 0\n",
    "        for _ in range(num_samples):\n",
    "            \n",
    "            temp_generation_dict = llm.generate(temp_mcse_prompt, max_new_tokens=512, return_type='generation_dict')\n",
    "            temp_pred_logprobs = llm.generate(temp_mcse_prompt, max_new_tokens=512, return_type='pred_logprobs')\n",
    "        \n",
    "            temp_new_ids = temp_generation_dict['new_ids']\n",
    "\n",
    "            temp_token_indices_split = token_indices_by_line(temp_new_ids)\n",
    "\n",
    "            temp_log_prob_list = []\n",
    "            for i, log_prob_tensor in enumerate(temp_pred_logprobs):\n",
    "                index = temp_new_ids[i]\n",
    "                temp_log_prob_list.append(temp_pred_logprobs[i][0, index].item())\n",
    "\n",
    "            # we only need the first sentence in the 'new_text'\n",
    "            line_logprobs = sum([temp_log_prob_list[i] for i in temp_token_indices_split[0]])\n",
    "            line_total_logprob += line_logprobs\n",
    "        \n",
    "        line_mcse = -line_total_logprob / num_samples\n",
    "        new_scores = {\"MCSE\": line_mcse}\n",
    "\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "\n",
    "        line_responce += line_text\n",
    "\n",
    "\n",
    "    return uncertainty_dict\n",
    "\n",
    "uncertainty_dict = mcse(uncertainty_dict, pred_logprobs)\n",
    "print('global_unc_score', uncertainty_dict['global_unc_score'])\n",
    "print('line_unc_scores', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnse(uncertainty_dict, pred_logprobs):\n",
    "    \n",
    "    input_text = uncertainty_dict['input_text']\n",
    "    new_ids = uncertainty_dict['new_ids']\n",
    "\n",
    "    num_samples = 3\n",
    "\n",
    "    total_normalized_logprob = 0\n",
    "    for _ in range(num_samples):\n",
    "        # Generate predicted log probabilities for each token in the sequence\n",
    "        pred_logprobs = llm.generate(input_text, max_new_tokens=512, return_type='pred_logprobs')\n",
    "\n",
    "        log_prob_list = []\n",
    "        sequence_length = 0  # To track the length of each generated sequence\n",
    "\n",
    "        # Calculate log probabilities for each generated token\n",
    "        for i, log_prob_tensor in enumerate(pred_logprobs):\n",
    "            index = token_ids[i]  # Index of the current token in the model's vocabulary\n",
    "            log_prob_list.append(log_prob_tensor[0, index].item())\n",
    "            sequence_length += 1  # Increment the sequence length for each token processed\n",
    "\n",
    "        # Sum the log probabilities for the current sequence\n",
    "        temp_logprob = sum(log_prob_list)\n",
    "\n",
    "        # Normalize the summed log probabilities by the sequence length\n",
    "        if sequence_length > 0:  # Avoid division by zero\n",
    "            normalized_logprob = temp_logprob / sequence_length\n",
    "            total_normalized_logprob += normalized_logprob\n",
    "\n",
    "    # Compute the Monte Carlo Normalized Sequence Entropy\n",
    "    global_mcnse = -total_normalized_logprob / num_samples\n",
    "        \n",
    "    if 'global_unc_score' not in uncertainty_dict:\n",
    "        uncertainty_dict['global_unc_score'] = {}\n",
    "    uncertainty_dict['global_unc_score'].update({'MCNSE': global_mcnse})\n",
    "\n",
    "    # Linewise ptrue scores\n",
    "    # Split the text into lines\n",
    "    line_split, _,_,_ = split_lines(new_ids)\n",
    "\n",
    "    line_responce = ''\n",
    "    token_indices_split = token_indices_by_line(new_ids)\n",
    "    for i,line_indices in enumerate(token_indices_split):\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_text = line_split[i]\n",
    "\n",
    "        print('line_text', line_text)\n",
    "\n",
    "        temp_mcse_prompt = generate_mcse_prompt(input_text, line_responce)\n",
    "\n",
    "        line_total_normalized_logprob = 0\n",
    "        for _ in range(num_samples):\n",
    "            # Generate a generation dictionary and corresponding log probabilities\n",
    "            temp_generation_dict = llm.generate(temp_mcse_prompt, max_new_tokens=512, return_type='generation_dict')\n",
    "            temp_pred_logprobs = llm.generate(temp_mcse_prompt, max_new_tokens=512, return_type='pred_logprobs')\n",
    "            \n",
    "            # Extract new token IDs from the generation dictionary\n",
    "            temp_new_ids = temp_generation_dict['new_ids']\n",
    "\n",
    "            # Split the token IDs by line to process each line individually\n",
    "            temp_token_indices_split = token_indices_by_line(temp_new_ids)\n",
    "\n",
    "            # Initialize a list to store log probabilities\n",
    "            temp_log_prob_list = []\n",
    "            for i, log_prob_tensor in enumerate(temp_pred_logprobs):\n",
    "                index = temp_new_ids[i]\n",
    "                temp_log_prob_list.append(temp_pred_logprobs[i][0, index].item())\n",
    "\n",
    "            # Calculate log probabilities for the first sentence in the 'new_text'\n",
    "            line_logprobs = sum([temp_log_prob_list[i] for i in temp_token_indices_split[0]])\n",
    "            line_length = len(temp_token_indices_split[0])  # Length of the first line\n",
    "\n",
    "            # Normalize the log probabilities by the line length if the line length is not zero\n",
    "            if line_length > 0:\n",
    "                normalized_line_logprobs = line_logprobs / line_length\n",
    "                line_total_normalized_logprob += normalized_line_logprobs\n",
    "\n",
    "        # Compute the Monte Carlo Normalized Sequence Entropy for the line\n",
    "        line_mcnse = -line_total_normalized_logprob / num_samples\n",
    "        new_scores = {\"MCNSE\": line_mcnse}\n",
    "\n",
    "        existing_line = next((item for item in uncertainty_dict['line_unc_scores'] if item[0] == line_text), None)\n",
    "        if existing_line:\n",
    "            existing_line[1].update(new_scores)\n",
    "        else:\n",
    "            uncertainty_dict['line_unc_scores'].append((line_text, new_scores))\n",
    "\n",
    "        line_responce += line_text\n",
    "\n",
    "\n",
    "    return uncertainty_dict\n",
    "\n",
    "uncertainty_dict = mcnse(uncertainty_dict, pred_logprobs)\n",
    "print('global_unc_score', uncertainty_dict['global_unc_score'])\n",
    "print('line_unc_scores', uncertainty_dict['line_unc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5852 True\n"
     ]
    }
   ],
   "source": [
    "id = llm.tokenizer('True')['input_ids'][1]\n",
    "text = llm.tokenizer.decode([5852])\n",
    "\n",
    "print(id, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
