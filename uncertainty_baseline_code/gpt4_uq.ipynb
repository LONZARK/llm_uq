{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_tokens(completion):\n",
    "    return [obj.token for obj in completion.choices[0].logprobs.content]\n",
    "\n",
    "def split_lines(tokens):\n",
    "    token_split = []\n",
    "    cur_split = []\n",
    "\n",
    "    for token in tokens:\n",
    "        cur_split.append(token)\n",
    "        if '\\n' in token:\n",
    "            token_split.append(cur_split)\n",
    "            cur_split = []\n",
    "\n",
    "    # Append any remaining tokens that did not end with a newline\n",
    "    if cur_split:\n",
    "        token_split.append(cur_split)\n",
    "\n",
    "    # print(\"Total tokens after split:\", sum(len(split) for split in token_split))\n",
    "\n",
    "    # Create list of lines by joining tokens and removing newlines\n",
    "    line_split = [''.join(split).replace('\\n', '') for split in token_split]\n",
    "\n",
    "    return line_split, token_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_indices_by_line(tokens):\n",
    "    line_split, token_split = split_lines(tokens)\n",
    "    token_indices_split =[]\n",
    "    for i, split in enumerate(token_split):\n",
    "        # print(token_split[i])\n",
    "        start_index = sum(len(token_split[j]) for j in range(i))\n",
    "        token_indices = [k for k in range(start_index, start_index + len(split))]\n",
    "        token_indices_split.append(token_indices)\n",
    "    return token_indices_split\n",
    "\n",
    "\n",
    "def code_token_indices_by_line(tokens, lang=\"python\"):\n",
    "    \"\"\"\n",
    "    extract the code lines from the tokens, output the token index of each line of code:\n",
    "    suppose there are two code lines, the first consists of token indexed 3 to 7, the second consists of token indexed 8 to 10.\n",
    "    The following list will be returned:\n",
    "    [[3,4,5,6,7],[8,9,10]]\n",
    "\n",
    "    \"\"\"\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "    line_split, token_split = split_lines(tokens)\n",
    "    code_flag = False\n",
    "    code=\"\"\n",
    "    prefix = \"```\"+lang\n",
    "    code_token_indices_split = []\n",
    "    for i, line in enumerate(line_split):\n",
    "        if line.startswith(prefix):\n",
    "            code_flag=True\n",
    "        elif line.startswith(\"```\"):\n",
    "            code_flag=False\n",
    "            break\n",
    "        elif code_flag==True:\n",
    "            code_token_indices_split.append(token_indices_split[i])\n",
    "    return code_token_indices_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy as cal_entropy\n",
    "\n",
    "\n",
    "def get_token_logprobs(completion):\n",
    "    token_logprobs = []\n",
    "    for token_logprob_obj in completion.choices[0].logprobs.content:\n",
    "        token_logprobs.append((token_logprob_obj.token, token_logprob_obj.logprob))\n",
    "    return token_logprobs\n",
    "\n",
    "def get_sequence_logprob(completion):\n",
    "    token_logprobs= get_token_logprobs(completion)\n",
    "    return np.sum([prob for (_, prob) in token_logprobs])\n",
    "\n",
    "\n",
    "def get_token_top_logprobs(completion):\n",
    "    \"\"\"\n",
    "    return a list of tuples of (token, top_logprobs), indicating the primary token and the log-probs for its top choices.\n",
    "    example tuple:\n",
    "    ('```', [('```', -0.28346914), ('Here', -1.8964262), ('Below', -2.5428724), ('The', -4.9661055), ('Certainly', -5.1264677)])\n",
    "    \"\"\"\n",
    "    token_top_logprobs = []\n",
    "    for token_logprob_obj in completion.choices[0].logprobs.content:\n",
    "        primary_token = token_logprob_obj.token\n",
    "        top_logprobs = [(top_logprob_obj.token, top_logprob_obj.logprob) for top_logprob_obj in token_logprob_obj.top_logprobs]\n",
    "        token_top_logprobs.append((primary_token, top_logprobs))\n",
    "    return token_top_logprobs\n",
    "\n",
    "\n",
    "def normalize_probs(probs):\n",
    "    prob_factor = 1 / sum(probs)\n",
    "    if isinstance(probs, list):\n",
    "        return [prob_factor * p for p in probs]\n",
    "    else:\n",
    "        return prob_factor * probs\n",
    "\n",
    "\n",
    "def print_token_probs(completion):\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "    token_top_logprobs = get_token_top_logprobs(completion)\n",
    "    print(token_top_logprobs)\n",
    "    max_len = np.max([len(token) for (token, _) in token_logprobs])\n",
    "    print(\"=\"*25)\n",
    "    for (token, logprob), token_top_logprob_tup in zip(token_logprobs, token_top_logprobs):\n",
    "        token_top_logprob = [(tup[0], round(np.exp(tup[1]),6)) for tup in token_top_logprob_tup[1]]\n",
    "        print(f\"| %12s | %.6f | %s\"%(repr(token), np.exp(logprob), str(token_top_logprob)))\n",
    "    print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_entropies(completion):\n",
    "    token_top_logprobs = get_token_top_logprobs(completion)\n",
    "    token_entropies = []\n",
    "    for (token, top_logprobs) in token_top_logprobs:\n",
    "        probs = [np.exp(top_logprob[1]) for top_logprob in top_logprobs]\n",
    "        probs_nm = normalize_probs(probs)\n",
    "        entropy = cal_entropy(probs_nm)\n",
    "        token_entropies.append((token, entropy))\n",
    "    return token_entropies\n",
    "\n",
    "\n",
    "def cal_MTE_global(completion, print_all=False):\n",
    "    token_entropies = get_token_entropies(completion)\n",
    "    # mean_token_entropy = np.mean(list(token_entropies.values()))\n",
    "    # Extract only the entropy values from the list of tuples\n",
    "    entropy_values = [entropy for token, entropy in token_entropies]\n",
    "    mean_token_entropy = np.mean(entropy_values)  # Calculate the mean of the entropy values\n",
    "\n",
    "    if print_all:\n",
    "        # print(f\"<token entropy>\")\n",
    "        # print(\"=\"*25)\n",
    "        # for token, entropy in token_entropies:\n",
    "        #     print(f\"| %12s | %.6f\"%(token, entropy))\n",
    "        # print(\"=\"*25)\n",
    "        print(f\"Mean Token Entropy: %.6f\"%(mean_token_entropy))\n",
    "    return mean_token_entropy\n",
    "\n",
    "def cal_MTE(completion, print_all=False):\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_entropies = get_token_entropies(completion)\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "\n",
    "    lines_info = []\n",
    "    for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        line_tokens = [tokens[i] for i in line_indices]\n",
    "        line = ''.join(line_tokens)\n",
    "        token_entropies_line = []\n",
    "        for index in line_indices:\n",
    "            token_entropies_line.append(token_entropies[index][1])\n",
    "        mte_score = np.mean(token_entropies_line)\n",
    "        lines_info.append((line, mte_score))\n",
    "\n",
    "    if print_all:\n",
    "        print(f\"<Mean Token Entropy by line>\")\n",
    "        print(\"=\"*25)\n",
    "        max_length = max(len(repr(line)) for line, _ in lines_info)\n",
    "        for (line, line_entropy) in lines_info:\n",
    "            print(f\"| {repr(line):<{max_length}} | {line_entropy:.6f}\")\n",
    "        print(\"=\"*25)\n",
    "    return lines_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_MSP_global(completion, print_all=False):\n",
    "    \"\"\"\n",
    "    Calculate the Maximum Softmax Probability (MSP) uncertainty score for the completion.\n",
    "    \"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "    token_logprobs = [logprob for _, logprob in token_logprobs]\n",
    "\n",
    "    token_logprobs = np.array(token_logprobs, dtype=float)  # Convert to numpy array of type float\n",
    "  \n",
    "    all_token_logprobs = np.sum(token_logprobs) \n",
    "    all_token_prob = np.exp(all_token_logprobs)\n",
    "    msp_score = 1 - all_token_prob\n",
    "\n",
    "    return msp_score\n",
    "\n",
    "def cal_MSP(completion, print_all=False):\n",
    "    \"\"\"\n",
    "    Calculate the Maximum Softmax Probability (MSP) uncertainty score for each line of the completion.\n",
    "\n",
    "    Steps:\n",
    "    1. For each line, calculate the MSP uncertainty score.\n",
    "    Suppose the prompt is x and the response y contains M lines, y(1), y(2), ... y(M), then the MSP uncertainty scores are represented as follows:\n",
    "    MSP(y(m)|y(<m),x,) = 1 - P(y(m)|x,)\n",
    "    P(y(m)|x,) = prod(l=1 to L) P(yl(m)|y<l(m),y(<m),x,), where L is the length of y(m)\n",
    "\n",
    "    Args:\n",
    "    completion: The completion object containing logprobs and content\n",
    "    print_all: Boolean flag to print intermediate results\n",
    "\n",
    "    Returns:\n",
    "    A list of length M that contains the MSP scores for y(1), y(2), ... y(M)\n",
    "    [MSP(y(1)|y(<1),x,), MSP(y(2)|y(<2),x,), â€¦, MSP(y(M)|y(<M),x,)]\n",
    "    \"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "    msp_scores = []\n",
    "    lines_info = []\n",
    "    for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        line_tokens = [tokens[i] for i in line_indices]\n",
    "        line = ''.join(line_tokens)\n",
    "        line_logprob = 0.0\n",
    "\n",
    "        for index in line_indices:\n",
    "            _, logprob = token_logprobs[index]\n",
    "            line_logprob += logprob\n",
    "\n",
    "        line_prob = np.exp(line_logprob)\n",
    "        msp_score = 1 - line_prob\n",
    "        msp_scores.append(msp_score)\n",
    "        lines_info.append((line, msp_score))\n",
    "\n",
    "    if print_all:\n",
    "        print(f\"<Lines, Line probabilities, and MSP scores>\")\n",
    "        print(\"=\"*60)\n",
    "        max_length = max(len(repr(line)) for line, _ in lines_info)\n",
    "        for (line, msp_score) in lines_info:\n",
    "            print(f\"| {repr(line):<{max_length}} | {msp_score:.6f}\")\n",
    "        print(\"=\"*60)\n",
    "    return lines_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PP(Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cal_PP_global(completion, print_all=False):\n",
    "    \"\"\"\n",
    "    Calculate the Perplexity (PP) uncertainty score for each line of the completion.\n",
    "\n",
    "    Steps:\n",
    "    1. For each line, calculate the PP uncertainty score.\n",
    "    Suppose the prompt is x and the response y contains M lines, y(1), y(2), ... y(M), then the PP uncertainty scores are represented as follows:\n",
    "    PP(y(m)|y(<m),x,) = 2^(-1/L * sum(l=1 to L) log2[P(yl(m)|y<l(m),y(<m),x,)])\n",
    "    where L is the length of y(m)\n",
    "\n",
    "    Args:\n",
    "    completion: The completion object containing logprobs and content\n",
    "    print_all: Boolean flag to print intermediate results\n",
    "\n",
    "    Returns:\n",
    "    A list of length M that contains the PP scores for y(1), y(2), ... y(M)\n",
    "    [PP(y(1)|y(<1),x,), PP(y(2)|y(<2),x,), â€¦, PP(y(M)|y(<M),x,)]\n",
    "    \"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "\n",
    "    line_logprobs = []\n",
    "    for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        for index in line_indices:\n",
    "            if index < len(token_logprobs):\n",
    "                _, logprob = token_logprobs[index]\n",
    "                line_logprobs.append(logprob)\n",
    "    avg_log_prob = np.mean(line_logprobs)\n",
    "    avg_log2_prob = avg_log_prob / np.log(2)\n",
    "    pp_score = 2 ** (-avg_log2_prob)\n",
    "\n",
    "    return pp_score\n",
    "\n",
    "def cal_PP(completion, print_all=False):\n",
    "    \"\"\"\n",
    "    Calculate the Perplexity (PP) uncertainty score for each line of the completion.\n",
    "\n",
    "    Steps:\n",
    "    1. For each line, calculate the PP uncertainty score.\n",
    "    Suppose the prompt is x and the response y contains M lines, y(1), y(2), ... y(M), then the PP uncertainty scores are represented as follows:\n",
    "    PP(y(m)|y(<m),x,) = 2^(-1/L * sum(l=1 to L) log2[P(yl(m)|y<l(m),y(<m),x,)])\n",
    "    where L is the length of y(m)\n",
    "\n",
    "    Args:\n",
    "    completion: The completion object containing logprobs and content\n",
    "    print_all: Boolean flag to print intermediate results\n",
    "\n",
    "    Returns:\n",
    "    A list of length M that contains the PP scores for y(1), y(2), ... y(M)\n",
    "    [PP(y(1)|y(<1),x,), PP(y(2)|y(<2),x,), â€¦, PP(y(M)|y(<M),x,)]\n",
    "    \"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "    pp_scores = []\n",
    "    lines_info = []\n",
    "    for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_tokens = [tokens[i] for i in line_indices]\n",
    "        line = ''.join(line_tokens)\n",
    "        L = len(line_indices)\n",
    "        if L == 0:\n",
    "            continue\n",
    "        line_logprobs = []\n",
    "        for index in line_indices:\n",
    "            if index < len(token_logprobs):\n",
    "                _, logprob = token_logprobs[index]\n",
    "                line_logprobs.append(logprob)\n",
    "        avg_log_prob = np.mean(line_logprobs)\n",
    "        # Convert from natural log to log base 2\n",
    "        avg_log2_prob = avg_log_prob / np.log(2)\n",
    "        pp_score = 2 ** (-avg_log2_prob)\n",
    "        pp_scores.append(pp_score)\n",
    "        lines_info.append((line, pp_score))\n",
    "\n",
    "    if print_all:\n",
    "        print(f\"<Lines, Average log2 probabilities, and PP scores>\")\n",
    "        print(\"=\"*70)\n",
    "        max_length = max(len(repr(line)) for line, _ in lines_info)\n",
    "        for (line, pp_score) in lines_info:\n",
    "            print(f\"| {repr(line):<{max_length}} | {pp_score:.6f}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    return lines_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pTrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def gen_verification_prompt(original_user_massage, original_response, sentence, task_type='sentence'):\n",
    "    \"\"\"Generate a verification prompt for a given sentence.\"\"\"\n",
    "\n",
    "    verification_user_message = f\"\"\"Question: {original_user_massage}\n",
    "Proposed Answer: {original_response}\n",
    "Is the {task_type} \"{sentence}\" in the proposed answer correct?\n",
    "(A) True\n",
    "(B) False\n",
    "Answer True or False without explanation:\"\"\"\n",
    "\n",
    "    verification_chat = [\n",
    "        {\"role\": \"user\", \"content\": verification_user_message}\n",
    "      ]\n",
    "\n",
    "    return verification_chat\n",
    "\n",
    "\n",
    "def cal_pTrue_local(completion):\n",
    "    \"\"\"Calculate pTrue based on the completion.\"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    token_top_logprobs = get_token_top_logprobs(completion)\n",
    "    token_logprobs = get_token_logprobs(completion)\n",
    "\n",
    "    for (primary_token, log_prob) in token_logprobs:\n",
    "        if \"true\" in primary_token.lower():\n",
    "            return np.exp(log_prob)\n",
    "\n",
    "    for primary_token, top_logprobs in token_top_logprobs:\n",
    "        true_prob = next((prob for token, prob in top_logprobs if \"true\" in token.lower()), None)\n",
    "        if true_prob is not None:\n",
    "            return np.exp(true_prob)  # Convert log probability to probability\n",
    "\n",
    "    return 0\n",
    "\n",
    "def cal_pTrue_global(client, original_prompt, completion, task_type='sentence', print_all=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate P(True) uncertainty for whole sentence in the response.\n",
    "\n",
    "    Args:\n",
    "    original_prompt: The original prompt given to the model\n",
    "    completion: The primary response completion object\n",
    "    task_type: The type of task ('sentence', 'line', or 'code')\n",
    "    print_all: Boolean flag to print intermediate results\n",
    "\n",
    "    Returns:\n",
    "    A pTrue score for whole sentence in the response\n",
    "    \"\"\"\n",
    "    original_response = ''.join(get_generated_tokens(completion))\n",
    "    verification_prompt = gen_verification_prompt(original_prompt, original_response, original_response, task_type)\n",
    "    verification_completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages= verification_prompt,\n",
    "      temperature=0,\n",
    "      logprobs=True,\n",
    "      top_logprobs=5,\n",
    "      max_tokens=512\n",
    "    )\n",
    "    pTrue_score = cal_pTrue_local(verification_completion)\n",
    "\n",
    "    return pTrue_score\n",
    "\n",
    "def cal_pTrue(client, original_prompt, completion, task_type='sentence', print_all=False):\n",
    "    \"\"\"\n",
    "    Calculate P(True) uncertainty for each sentence in the response.\n",
    "\n",
    "    Args:\n",
    "    original_prompt: The original prompt given to the model\n",
    "    completion: The primary response completion object\n",
    "    task_type: The type of task ('sentence', 'line', or 'code')\n",
    "    print_all: Boolean flag to print intermediate results\n",
    "\n",
    "    Returns:\n",
    "    A list of pTrue scores for each sentence in the response\n",
    "    \"\"\"\n",
    "    tokens = get_generated_tokens(completion)\n",
    "    original_response = ''.join(get_generated_tokens(completion))\n",
    "    line_split, token_split = split_lines(tokens)\n",
    "    pTrue_scores = []\n",
    "    \n",
    "    lines_info = []\n",
    "    token_indices_split = token_indices_by_line(tokens)\n",
    "    for line_indices in token_indices_split:\n",
    "        if not line_indices:  # Skip empty lines\n",
    "            continue\n",
    "        line_tokens = [tokens[i] for i in line_indices]\n",
    "        line = ''.join(line_tokens)\n",
    "        L = len(line_indices)\n",
    "        if L == 0:\n",
    "            continue\n",
    "    # for line_sentence in line_split:\n",
    "        verification_prompt = gen_verification_prompt(original_prompt, original_response, line, task_type)\n",
    "        # print('verification_prompt', verification_prompt)\n",
    "        # print('sentence', line_sentence)\n",
    "        verification_completion = client.chat.completions.create(\n",
    "          model=\"gpt-4-turbo-2024-04-09\",\n",
    "          messages= verification_prompt,\n",
    "          temperature=0,\n",
    "          logprobs=True,\n",
    "          top_logprobs=5,\n",
    "          max_tokens=512\n",
    "        )\n",
    "        pTrue = cal_pTrue_local(verification_completion)\n",
    "        pTrue_scores.append(pTrue)\n",
    "\n",
    "        lines_info.append((line, pTrue))\n",
    "\n",
    "    if print_all:\n",
    "        print(f\"<Sentences and P(True) scores>\")\n",
    "        print(\"=\"*70)\n",
    "        max_length = max(len(repr(line_sentence)) for line_sentence, _ in lines_info)\n",
    "        for (line_sentence, pTrue) in lines_info:\n",
    "            print(f\"| {repr(line_sentence):<{max_length}} | {pTrue:.6f}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    return lines_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity (Code vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def code_vectorized_similarity(code1, code2, method=\"Cosine\", embedding_type=\"CodeBert\"):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two code segments using the specified method and embedding type.\n",
    "\n",
    "    Args:\n",
    "    code1, code2: the two code segments, each is a string\n",
    "    method: \"Cosine\" or \"Euclidean\", the method to calculate the code similarity in vector space\n",
    "    embedding_type: \"CodeBert\" or \"GraphCodeBert\", the embedding method\n",
    "\n",
    "    Returns:\n",
    "    A scalar that represents the similarity score of code1 and code2\n",
    "    \"\"\"\n",
    "    # Choose the appropriate model based on the embedding type\n",
    "    if embedding_type == \"CodeBert\":\n",
    "        model_name = \"microsoft/codebert-base\"\n",
    "    elif embedding_type == \"GraphCodeBert\":\n",
    "        model_name = \"microsoft/graphcodebert-base\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding type. Choose 'CodeBert' or 'GraphCodeBert'.\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize and generate embeddings\n",
    "    def get_embedding(code):\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    embedding1 = get_embedding(code1)\n",
    "    embedding2 = get_embedding(code2)\n",
    "\n",
    "    # Calculate similarity based on the specified method\n",
    "    if method == \"Cosine\":\n",
    "        similarity = torch.nn.functional.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()\n",
    "\n",
    "    elif method == \"Euclidean\":\n",
    "        similarity = 1 / (1 + torch.norm(embedding1 - embedding2)).item()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'Cosine' or 'Euclidean'.\")\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# # Example code snippets\n",
    "# code1 = \"def sum(a, b): return a + b\"\n",
    "# code2 = \"def add(x, y): return x + y\"\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# cosine_sim_codebert = code_vectorized_similarity(code1, code2, method=\"Cosine\", embedding_type=\"CodeBert\")\n",
    "# cosine_sim_graphcodebert = code_vectorized_similarity(code1, code2, method=\"Cosine\", embedding_type=\"GraphCodeBert\")\n",
    "# euclidean_sim_codebert = code_vectorized_similarity(code1, code2, method=\"Euclidean\", embedding_type=\"CodeBert\")\n",
    "# euclidean_sim_graphcodebert = code_vectorized_similarity(code1, code2, method=\"Euclidean\", embedding_type=\"GraphCodeBert\")\n",
    "# print(\"Cosine Similarity (CodeBert):\", cosine_sim_codebert)\n",
    "# print(\"Cosine Similarity (GraphCodeBERT):\", cosine_sim_graphcodebert) # .item()\n",
    "# print(\"Euclidean Similarity (CodeBert):\", euclidean_sim_codebert)\n",
    "# print(\"Euclidean Similarity (GraphCodeBERT):\", euclidean_sim_graphcodebert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simailarity (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_semantic_similarity_prompt(code_1, code_2):\n",
    "    prompt = f\"\"\"\n",
    "I am comparing two pieces of Python code and need to determine how similar they are. Consider only the functionality and semantics. Provide a similarity score between 0 and 1, where 0 means completely different and 1 means exactly the same.\n",
    "\n",
    "Code 1:\n",
    "{code_1}\n",
    "\n",
    "Code 2:\n",
    "{code_2}\n",
    "\n",
    "Please analyze the similarity and provide a score without explanation, use the following format: semantic similarity score =\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_general_similarity_prompt(code_1, code_2):\n",
    "    prompt = f\"\"\"\n",
    "I am comparing two pieces of Python code and need to determine how similar they are. Consider both their functionality and style. Provide a similarity score between 0 and 1, where 0 means completely different and 1 means exactly the same.\n",
    "\n",
    "Code 1:\n",
    "{code_1}\n",
    "\n",
    "Code 2:\n",
    "{code_2}\n",
    "\n",
    "Please analyze the similarity and provide a score without explanation, use the following format: similarity score =\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_similarity_score(response):\n",
    "    # Use regex to find the similarity score in the response\n",
    "    match = re.search(r'similarity score\\s*=\\s*(\\d+(\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def code_LLM_similarity(code1, code2, type='semantic'):\n",
    "    if type == 'semantic':\n",
    "        prompt = generate_semantic_similarity_prompt(code1, code2)\n",
    "    else:\n",
    "        prompt = generate_general_similarity_prompt(code1, code2)\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages= chat,\n",
    "      temperature=0,\n",
    "      logprobs=True,\n",
    "      top_logprobs=5,\n",
    "      max_tokens=512\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    similarity_score = extract_similarity_score(response)\n",
    "\n",
    "    print(f\"<type>\\n{type}\")\n",
    "    print(f\"<user>\\n{prompt}\")\n",
    "    print(f\"<assistant>\\n{completion.choices[0].message.content}\")\n",
    "    print(f\"<similarity_score>\\n{similarity_score}\")\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage\n",
    "from openai import OpenAI\n",
    "\n",
    "# code_1 = '''def add(a, b):\n",
    "#     return a + b'''\n",
    "# code_2 = '''def sum_numbers(x, y):\n",
    "#     return x + y'''\n",
    "\n",
    "# print('===========================')\n",
    "# code_LLM_similarity(code_1, code_2, type='semantic')\n",
    "# print('===========================')\n",
    "# code_LLM_similarity(code_1, code_2, type='general')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task July-22(Jia) Generate the uncertainty scores for each line for the first 40 problems in HumanEval plus dataset, with gpt-turbo-4- and gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_to_dict(scores):\n",
    "    # Initialize dictionary to hold line-wise uncertainty scores\n",
    "    line_uncertainty_dict = {}\n",
    "    # Populate the dictionary\n",
    "    for method, data in scores.items():\n",
    "        for (line, score) in data:\n",
    "            if line not in line_uncertainty_dict:\n",
    "                line_uncertainty_dict[line] = {}\n",
    "            line_uncertainty_dict[line][method] = round(score, 3)  # Round the score to 3 decimal places\n",
    "    return line_uncertainty_dict\n",
    "\n",
    "def add_score_to_list(scores):\n",
    "    # Initialize list to hold line-wise uncertainty scores\n",
    "    line_uncertainty_list = []\n",
    "\n",
    "    # Helper function to find an existing entry by line\n",
    "    def find_entry_by_line(target_line):\n",
    "        for entry in line_uncertainty_list:\n",
    "            if entry[0] == target_line:\n",
    "                return entry\n",
    "        return None\n",
    "\n",
    "    # Populate the list\n",
    "    for method, data in scores.items():\n",
    "        for (line, score) in data:\n",
    "            entry = find_entry_by_line(line)\n",
    "            if entry is None:\n",
    "                # If no entry exists, create one with this line and an empty dictionary for scores\n",
    "                entry = (line, {})\n",
    "                line_uncertainty_list.append(entry)\n",
    "            # Add or update the score for the current method\n",
    "            entry[1][method] = round(score, 3)  # Round the score to 3 decimal places\n",
    "\n",
    "    return line_uncertainty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_line_unc_scores(line_unc_scores):\n",
    "    if not line_unc_scores:\n",
    "        return\n",
    "\n",
    "    # Determine the width for the line content column\n",
    "    line_content_width = max(len(repr(line)) for line, _ in line_unc_scores) + 2\n",
    "\n",
    "    # Determine the width for each uncertainty score column\n",
    "    score_keys = line_unc_scores[0][1].keys()\n",
    "    print('score_keys', score_keys)\n",
    "    score_widths = {key: max(len(key), max(len(f\"{scores[key]:.3f}\") for _, scores in line_unc_scores)) for key in score_keys}\n",
    "\n",
    "    # Adjust total width and print the header\n",
    "    total_width = line_content_width + sum(score_widths.values()) + 3 * len(score_keys) + 2 * (len(score_keys) - 1)\n",
    "    print(\"=\" * total_width)\n",
    "    header = f\"{'Line Content':<{line_content_width}}\"\n",
    "    header += ''.join(f\"| {key:^{score_widths[key]}} \" for key in score_keys)\n",
    "    print(header)\n",
    "    print(\"=\" * total_width)\n",
    "    \n",
    "    # Print each line with its uncertainty scores\n",
    "    for line, scores in line_unc_scores:\n",
    "        row = f\"{repr(line):<{line_content_width}}\"\n",
    "        row += ''.join(f\"| {scores[key]:>{score_widths[key]}.3f} \" for key in score_keys)\n",
    "        print(row)\n",
    "\n",
    "    # Print the footer\n",
    "    print(\"=\" * total_width)\n",
    "\n",
    "def save_to_txt(user_message, task_id, line_unc_scores, file_path):\n",
    "    if not line_unc_scores:\n",
    "        return  # If the list is empty, return without doing anything\n",
    "\n",
    "    # Determine the width for the line content column\n",
    "    line_content_width = max(len(repr(line)) for line, _ in line_unc_scores) + 2\n",
    "\n",
    "    # Determine the width for each uncertainty score column\n",
    "    score_keys = line_unc_scores[0][1].keys() if line_unc_scores else []\n",
    "    score_widths = {key: max(len(key), max(len(f\"{scores[key]:.3f}\") for _, scores in line_unc_scores)) for key in score_keys}\n",
    "\n",
    "    # Open the file for writing\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write user message and task id\n",
    "        file.write(f\"Task ID: {task_id}\\n\\n\")\n",
    "        file.write(f\"User Message: {user_message}\\n\")\n",
    "\n",
    "        # Print the header\n",
    "        total_width = line_content_width + sum(score_widths.values()) + 3 * len(score_keys) + 2 * (len(score_keys) - 1)\n",
    "        file.write(\"=\" * total_width + \"\\n\")\n",
    "        header = f\"{'Generate Content':<{line_content_width}}\"\n",
    "        header += ''.join(f\"| {key:^{score_widths[key]}} \" for key in score_keys)\n",
    "        file.write(header + \"\\n\")\n",
    "        file.write(\"=\" * total_width + \"\\n\")\n",
    "        \n",
    "        # Print each line with its uncertainty scores\n",
    "        for line, scores in line_unc_scores:\n",
    "            row = f\"{repr(line):<{line_content_width}}\"\n",
    "            row += ''.join(f\"| {scores[key]:>{score_widths[key]}.3f} \" for key in score_keys)\n",
    "            file.write(row + \"\\n\")\n",
    "\n",
    "        # Print the footer\n",
    "        file.write(\"=\" * total_width + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_jsonDict(temp_data, final_prompt, response, unc_scores, line_unc_scores)\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['task_id'] = temp_data['task_id']\n",
    "    temp_dict['prompt'] = temp_data['prompt']\n",
    "    temp_dict['canonical_solution'] = temp_data['canonical_solution']\n",
    "    temp_dict['entry_point'] = temp_data['entry_point']\n",
    "    temp_dict['test'] = temp_data['test']\n",
    "    temp_dict['final_prompt'] = final_prompt\n",
    "    temp_dict['response'] = response\n",
    "    temp_dict['unc_scores'] = unc_scores\n",
    "    temp_dict['line_unc_scores'] = line_unc_scores\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "def save_to_json(data_list, save_path):\n",
    "    # Writing the list of dictionaries to a JSON file\n",
    "    with open(save_path, 'w') as json_file:\n",
    "        json.dump(data_list, json_file, indent=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "def main(model_id, client, subsample_size = 1):\n",
    "    system_message = '''You are a Python code generator. Generate a complete and functioning Python function based on the provided code snippet.\n",
    "    Ensure the function includes the original instructions in the comments, in-line comments for each line of code, and import statements for any required dependencies.\n",
    "    Do not include main function. Enclose your code inside a ```python``` block.'''\n",
    "\n",
    "    dataset = load_dataset(\"evalplus/humanevalplus\")\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(subsample_size):\n",
    "\n",
    "        user_message = dataset['test'][i]['prompt']\n",
    "        task_id = dataset['test'][i]['task_id']\n",
    "\n",
    "        print('task_id', task_id)\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "          ]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "          model= model_id,\n",
    "          messages= chat,\n",
    "          temperature=0,\n",
    "          logprobs=True,\n",
    "          top_logprobs=5,\n",
    "          max_tokens=512\n",
    "        )\n",
    "\n",
    "        uq_methods = {\n",
    "            'PP': cal_PP(completion, print_all=False),\n",
    "            'MSP': cal_MSP(completion, print_all=False),\n",
    "            'pTrue': cal_pTrue(client, user_message, completion, task_type='sentence', print_all=False),\n",
    "            'MTE': cal_MTE(completion, print_all=False),\n",
    "        }\n",
    "\n",
    "        uq_global = {\n",
    "            'PP_global': cal_PP_global(completion, print_all=False),\n",
    "            'MSP_global': cal_MSP_global(completion, print_all=False),\n",
    "            'pTrue_global': cal_pTrue_global(client, user_message, completion, task_type='sentence', print_all=False),\n",
    "            'MTE_global': cal_MTE_global(completion, print_all=False)\n",
    "        }\n",
    "\n",
    "        scores = {}\n",
    "        for uq in list(uq_methods.keys()):\n",
    "            line_score = uq_methods[uq]\n",
    "            print(uq, 'line_score', line_score)\n",
    "            scores[uq] = line_score\n",
    "        line_uncertainty_list = add_score_to_list(scores)\n",
    "        # display_line_unc_scores(line_uncertainty_list)\n",
    "\n",
    "        output_text_path = f'/home/jxl220096/llm_uq/logs/{model_id}/task_id.txt'\n",
    "        save_to_txt(user_message, task_id, line_uncertainty_list, f'/home/jxl220096/llm_uq/logs/{model_id}/{task_id}.txt')\n",
    "\n",
    "        temp_dict = transfer_to_jsonDict(temp_data = dataset['test'][i], final_prompt = chat, response = completion.choices[0].message.content, unc_scores = uq_global, line_unc_scores = line_uncertainty_list)\n",
    "        data_list.append(temp_dict)\n",
    "    \n",
    "    output_json_path = f'/home/jxl220096/llm_uq/logs/{model_id}/{model_id}.json'\n",
    "    save_to_json(data_list, output_json_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
